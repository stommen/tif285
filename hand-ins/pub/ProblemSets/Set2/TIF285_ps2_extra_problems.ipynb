{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "- See deadline on the course web page\n",
    "- This problem set is solved individually. See examination rules on the course web page and the explanation of the examination procedure below.\n",
    "- The two notebooks for each problem set contain a number of basic and extra problems; you can choose which and how many to work on. The extra problems are usually more challenging.\n",
    "- Students are allowed to discuss together and help each other when solving the problems. However, every student must understand their submitted solution in the sense that they should be able to explain and discuss them with a peer or with a teacher.\n",
    "- While discussions with your peers are allowed (and even encouraged), direct plagiarism is not. Every student must reach their own understanding of submitted solutions according to the definition in the previous point.\n",
    "- The use of coding assistance from code generating artificial intelligence tools is allowed. However, every student must reach their own understanding of submitted solutions (including employed algorithms) according to the definition above.\n",
    "- Some problems include checkpoints in the form of `assert` statements. These usually check some basic functionality and you should make sure that your code passes these statements without raising an `AssertionError`. \n",
    "- Do not use other python modules than the ones included in the `environment.yml` file in the course github repo. \n",
    "\n",
    "- **Important:** The grading of problem sets requires **all** of the following actions:\n",
    "  1. Make sure to always complete **Task 0** in the header part of the notebook and that this part does not raise any `AssertionError`(s).\n",
    "  1. **Complete** the corresponding questions in Yata for every task that you have completed. This usually involves copying and pasting some code from your solution notebook and passing the code tests. You need to have a green check mark on Yata to get the corresponding points.\n",
    "  1. **Upload** your solution in the form of your edited version of this Jupyter notebook via the appropriate assignment module in Canvas (separate for basic and extra tasks). It is the code and results in your submitted notebook that is considered to be your hand-in solution.\n",
    "  1. If selected, be **available for a discussion** of your solution with one of the teachers on the Monday afternoon exercise session directly following the problem set deadline. No extra preparation is needed for these discussions apart from familiarity with your own solution. A list of randomly selected students will be published on the course web page around Monday noon. During the afternoon session that same day, students will be called in the numbered order until the end of the list (or the end of the exercise session). You must inform the responsible teacher as soon as possible following the publication of the student list if you can not be physically present at the exercise session (in which case we will have the discussion on zoom). An oral examination (on all aspects of the course) will be arranged during the exam week for students that do not show up for their discussion slot, or that fail to demonstrate familiarity with their hand-in solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "- Make sure that the **run time is smaller than a few minutes**. If needed you might have to reduce some computational tasks; e.g. by decreasing the number of grid points or sampling steps. Please ask the supervisors if you are uncertain about the run time. \n",
    "\n",
    "- Your solutions are usually expected where it says `YOUR CODE HERE` or <font color=\"red\">\"PLEASE WRITE YOUR ANSWER HERE\"</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0 \n",
    "#### (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "251105c400969a6d24e9f7ec4883888e",
     "grade": false,
     "grade_id": "cell-6f99a2583f9fb27d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "By changing the below boolean variable `student_self_assessment` to `True` you attest that:\n",
    "- All handed in solutions were produced by yourself in the sense that you understand your solutions and should be able to explain and discuss them with a peer or with a teacher.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64a7020fd0e62e5b51ef4470ae1c797f",
     "grade": false,
     "grade_id": "student-self-assessment",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "student_self_assessment = False\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d873afed15d2d3de2ef460d53fccf90f",
     "grade": true,
     "grade_id": "cell-795bedd2908899aa",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert student_self_assessment == True, 'You must assert the individual solution statements.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 2 (Extra problems)\n",
    "**Learning from data [TIF285], Chalmers, Fall 2024**\n",
    "\n",
    "Last revised: 18-Sep-2024 by Christian ForssÃ©n [christian.forssen@chalmers.se]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6874d6f0612340a685cc96f6c456b90b",
     "grade": false,
     "grade_id": "cell-bf9197ccd45cb935",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Data files are stored in\n",
    "DATA_DIR = \"DataFiles/\"\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "485c2ac5dabeb304effd1eb9eba55807",
     "grade": false,
     "grade_id": "cell-f66a1e93499bc542",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing modules\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#...\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Signal and background (3 points)\n",
    "The goal of this problem is to estimate the amplitude of a signal when there is a background.  We'll take a limiting case where the background is flat, so it is completely specified by its magnitude $B > 0$, and the signal is known to be a Gaussian with unknown amplitude $A$ but known position (mean) and width (standard deviation). \n",
    "\n",
    "The measurements will be integer numbers of counts $\\{N_k\\}$ in well-defined (equally spaced) bins $\\{x_k\\}$. The index $k$ runs over integers labeling the bins.\n",
    "\n",
    "We can imagine three different goals of the data analysis:\n",
    "- Finding $A$ and $B$ given $\\{N_k\\}$.\n",
    "- Finding $A$ (we do not care about $B$).\n",
    "- Finding $B$ (we do not care about $A$).\n",
    "\n",
    "In all cases we consider the bin sizes and the signal shape (including its mean position and width) as known information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our statistical model includes the true signal plus a constant background. The signal and the background magnitudes are the unknown parameters while the other parameters dictating the signal (width $w$ and mean $x_0$ of the Gaussian) are known and fixed:\n",
    "\n",
    "$$\n",
    "   D_k = n_0 \\left[ A e^{-(x_k-x_0)^2/2 w^2} + B \\right]\n",
    "$$\n",
    "\n",
    "Here $n_0$ is a constant that scales with measurement time.  Note that $D_k$ is not an integer in general, unlike $N_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "557c5334d34ef7524e18ab6706d57e25",
     "grade": false,
     "grade_id": "P5-modules",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# import statements.\n",
    "# We use pickle to save and load a python dictionary\n",
    "import pickle\n",
    "# factorial from the math module is useful. You might consider other modules as well.\n",
    "from math import factorial\n",
    "\n",
    "# import additional modules as needed\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates data according to the statistical model\n",
    "A_true = 1.\n",
    "B_true = 2.\n",
    "\n",
    "def exact_data(A, B, n_0, x_k, x_0=0., width=np.sqrt(5.)):\n",
    "    \"\"\"\n",
    "    Return the exact signal plus background.  The overall scale is n_0,\n",
    "    which is determined by how long counts are collected. \n",
    "    The default signal position and width are 0.0 and sqrt(5), respectively (in some  irrelevant units).\n",
    "    \"\"\"\n",
    "    return n_0 * (A * np.exp(-(x_k - x_0)**2/(2.*width**2)) + B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson distribution\n",
    "We are imagining a counting experiment, so the statistics of the counts we record will follow a Poisson distribution. It might be an interesting exercise to derive why this distribution is expected for a counting experiment. \n",
    "\n",
    "The Poisson discrete random variable from scipy.stats is defined by (see [documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.poisson.html))\n",
    "\n",
    "$$\n",
    "p(k \\mid \\mu) = \\frac{\\mu^k e^{-\\mu}}{k!} \\quad \\mbox{for }k\\geq 0 \\;.\n",
    "$$\n",
    "\n",
    "where $k$ is an integer and $\\mu$ is called the shape parameter. The mean and variance of this distribution are both equal to $\\mu$. Sivia and Gregory each use a different notation for for this distribution, which means you need to be flexible. \n",
    "\n",
    "For convenience, we'll define our own version in this notebook:\n",
    "\n",
    "$$\n",
    "p(N \\mid D) = \\frac{D^N e^{-D}}{N!} \\quad \\mbox{for }N\\geq 0 \\;.\n",
    "$$\n",
    "\n",
    "where $N$ is an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataset for exploring\n",
    "def make_dataset(A_true, B_true, width, x_0, databins=15, delta_x=1, D_max=100):\n",
    "    \"\"\"\n",
    "    Create a data set based on the number of bins (databins), the spacing\n",
    "    of bins (delta_x), and the maximum we want the exact result to have\n",
    "    (D_max, this fixes the n_0 parameter).\n",
    "    \n",
    "    Return arrays for the x points (xk_pts), the corresponding values of the\n",
    "    exact signal plus background in those bins (Dk_pts), the measured values\n",
    "    in those bins (Nk_pts, integers drawn from a Poisson distribution), the \n",
    "    maximum extent of the bins (x_max) and n_0.\n",
    "    \"\"\"\n",
    "    # set up evenly spaced bins, centered on x_0\n",
    "    x_max = x_0 + delta_x * (databins-1)/2\n",
    "    xk_pts = np.arange(-x_max, x_max + delta_x, delta_x, dtype=int)\n",
    "    \n",
    "    # scale n_0 so maximum of the \"true\" signal plus background is D_max\n",
    "    n_0 = D_max / (A_true + B_true)  \n",
    "    Dk_pts = exact_data(A_true, B_true, n_0, xk_pts, width=width)\n",
    "    \n",
    "    # sample for each k to determine the measured N_k\n",
    "    Nk_pts = [stats.poisson.rvs(mu=Dk) for Dk in Dk_pts]\n",
    "    \n",
    "    return xk_pts, Dk_pts, Nk_pts, x_max, n_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the signal and the data (this plotting task is not graded, but will help you understand the data scenarios)\n",
    "* Make a plot of the true signal plus background we are trying to deduce. Use $A_\\mathrm{true}=1$ and $B_\\mathrm{true}=2$ and the signal position (mean) $x_0=0$ and width (standard deviation)  $w=\\sqrt{5}$.\n",
    "\n",
    "We consider what happens for fixed signal and background but changing the experimental conditions specified by `D_max` and `databins` (we'll keep `delta_x` fixed to 1). In all cases the bins are symmetric around $x=0$.\n",
    "\n",
    "The pickle file that is loaded in the cell below contains data from four differently designed counting experiments.:\n",
    "1. Baseline case: 15 bins and maximum expection of 100 counts per bin.\n",
    "1. Low statistics case: 15 bins and maximum expection of only 10 counts per bin.\n",
    "1. Greater range case: 31 bins (with fixed bin width) and maximum expection of 50 counts per bin to give approximately the same total number of counts as in baseline case.\n",
    "1. Smaller range case: 7 bins (with fixed bin width) and maximum expection of 200 counts per bin to give approximately the same total number of counts as in baseline case.\n",
    " \n",
    "* Make four subplots that correspond to the data from the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0140c1a289a0dc749d6e71ee21f301f6",
     "grade": false,
     "grade_id": "P5-data-and-plots",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plotting the signal and background\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# \n",
    "\n",
    "# The data has been generated already and will be loaded from a pickle file.\n",
    "# It is a dictionary with four keys corresponding to the four cases, and each value\n",
    "# corresponding to xk_pts, Dk_pts, Nk_pts, x_max, n_0\n",
    "with open(f'{DATA_DIR}/PS2_Prob5_data.pickle','rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    print('Loaded \"data\" dictionary from file.')\n",
    "    print('Extract data with:')\n",
    "    print('xk_pts, Dk_pts, Nk_pts, x_max, n_0 = data[case]')\n",
    "    print('where the key \"case\" is one of:')\n",
    "    cases = data.keys()\n",
    "    print(cases)\n",
    "\n",
    "# Plotting the data for the four cases\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Compute and plot the joint posterior on a grid\n",
    "**Tasks**\n",
    "* Implement functions for the (log) likelihood. Let's use a uniform prior for $0 \\le A \\le 5$ and $0 \\le B \\le 5$.\n",
    "* Evaluate the log-posterior on a grid and then: \n",
    "  - Plot the joint posterior pdf for $A$ and $B$ for the four cases.\n",
    "  - Plot the marginalized posterior pdf for the signal amplitude $A$ for the four cases.\n",
    "  \n",
    "  Use the same axis scales for all four cases such that the precision of the inference can be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0ab59605a2e44ace69aa42f85b6cbdd",
     "grade": false,
     "grade_id": "P5a-PDFs",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the pdfs and combine with Bayes' theorem.\n",
    "\n",
    "def log_prior(A, B):\n",
    "    \"\"\"\n",
    "    Log prior .\n",
    "    \n",
    "    We take a uniform (flat) prior with large enough\n",
    "    maximums but, more importantly, require positive values for A and B.\n",
    "    \"\"\"\n",
    "    A_max = 5.\n",
    "    B_max = 5.\n",
    "    # flat prior \n",
    "    if np.logical_and(A <= A_max, B <= B_max).all(): \n",
    "        return np.log(1/(A_max * B_max))\n",
    "    else:\n",
    "        return -np.inf\n",
    "\n",
    "\n",
    "def log_likelihood(A, B, xk_pts, Nk_pts, n_0):\n",
    "    \"\"\"Log likelihood for data Nk_pts given A and B\"\"\"\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n",
    "    \n",
    "def log_posterior(A, B, xk_pts, Nk_pts, n_0):\n",
    "    \"\"\"Log posterior for data Nk_pts given A and B\"\"\"\n",
    "    return log_prior(A, B) + log_likelihood(A, B, xk_pts, Nk_pts, n_0)\n",
    "\n",
    "# Other utility code can be put here (if needed)\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1629a08952f75ed67dc6819122c29be",
     "grade": false,
     "grade_id": "P5a-posteriors",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Optional.\n",
    "# Code to find contour levels of gridded 2D posterior.\n",
    "\n",
    "def find_contour_levels(grid):\n",
    "    \"\"\"Compute 1, 2, 3-sigma contour levels for a gridded 2D posterior\n",
    "       Note: taken from BayesianAstronomy but may not work here.\n",
    "    \"\"\"\n",
    "    sorted_ = np.sort(grid.ravel())[::-1]\n",
    "    pct = np.cumsum(sorted_) / np.sum(sorted_)\n",
    "    cutoffs = np.searchsorted(pct, np.array([0.68, 0.95, 0.997]) ** 2)\n",
    "    return np.sort(sorted_[cutoffs])\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Design of experiment\n",
    "**Tasks** \n",
    "Answer the following questions. You can use the markdown cell below.\n",
    "  1. Can you understand why the signal and background amplitudes are anticorrelated? And why the (anti)correlation seems to be stronger in one of the cases? \n",
    "  1. What are your conclusions for how to design the experiment given limited resources? \n",
    "    - In particular, given that you wanted to be able to distinguish between signal amplitude and background, would it then be better to have many counts in few bins, or the same total amount of counts spread over a wider interval? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer here**\n",
    "\n",
    "1. Answer to question 1\n",
    "2. Answer to question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec776657f51d7b84a8119600b7a608e6",
     "grade": false,
     "grade_id": "P5b-expdesign",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Ignore this cell. No code is needed here.\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6: Good-data, bad-data\n",
    "### Task (a): 3 points; Task (b): 1 point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's start by defining some data that we will fit with a straight line.  The following data is measured velocities and distances for a set of galaxies. We will assume that there is an observational error on the $y$ values (and no error on $x$). \n",
    "- These errors are considered as independent and identically distributed (i.i.d.) from a normal distribution with mean 0.0 and standard deviation $\\sigma = 200$ km/sec.\n",
    "- However, as seen in the data table below, it turns out that two of the measurements resulted in \"bad data\" with errors that are much larger than what could be expected from the assumed error model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "518519da09df1054579ca39c028998cf",
     "grade": false,
     "grade_id": "cell-0579ba473ff606b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Data from student lab observations; \n",
    "# x = Galaxy distances in MPc\n",
    "# y = Galaxy velocity in km/sec\n",
    "x = np.array([10.1 ,45.2, 19.7, 31.2, 31.9, 44.0,\n",
    "       14.9, 35.1,  39.9  ])\n",
    "y = np.array([1507.9, 2937.5,  930.4, 2037.1, 2131.1,\n",
    "       2795.6, 1061.8, 2464.8, \n",
    "       1971.1])\n",
    "# Two of these samples, the 10.1 and 39.9 data points, were added by hand.\n",
    "# They are rather extreme samples from a Cauchy distribution with the same FWHM and \n",
    "# could be assumed to come from an experiment where some systematic error was missing from the analysis\n",
    "# (here leading the experimentalists to assign a Gaussian error when in fact it should have been Lorenzian).\n",
    "\n",
    "intercept = 0.\n",
    "slope = 70\n",
    "theta_true = [intercept, slope]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba4fed76f8f193381a7314210ba4ae58",
     "grade": false,
     "grade_id": "cell-b70037b8cfce4773",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the data with the assumed error model indicated by a standard deviation\n",
    "dy=200\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.errorbar(x, y, dy, fmt='o')\n",
    "ax.set_xlabel(r'distance [MPc]')\n",
    "ax.set_ylabel(r'velocity [km/sec]');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task (a)\n",
    "\n",
    "The question that we will be asking is: \n",
    "> What value would you infer for the Hubble constant, i.e. the slope of the velocity versus distance relation, given this data?\n",
    "\n",
    "We will make the prior assumption that the data can be fitted with a straight line. But we note that we are actually not interested in the offset of the straight line, but just its slope.\n",
    "\n",
    "We will try three different approaches:\n",
    "1. Maximum likelihood estimate\n",
    "1. Bayesian analysis\n",
    "1. Bayesian analysis incorporating a Bayesian approach to identify \"good-data-bad-data\"\n",
    "\n",
    "As a second task, we will also explore how the posterior belief from this analysis can feed into a subsequent data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical model\n",
    "Here we are given data with simple error bars (constant $\\sigma$), which implies that the probability for any *single* data point is a normal distribution about the true value. That is,\n",
    "\n",
    "$$\n",
    "y_i \\sim \\mathcal{N}(y_M(x_i;\\theta), \\sigma),\n",
    "$$\n",
    "\n",
    "with $y_M(x) = mx + b$.\n",
    "Or, in other words,\n",
    "\n",
    "$$\n",
    "p(y_i\\mid\\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(\\frac{-\\left[y_i - y_M(x_i;\\theta)\\right]^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is the (known) measurement error indicated by the error bars.\n",
    "\n",
    "ps Note that the assignment of normally distributed experimental errors is incorrect for two of the data points, but we don't know that (yet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Maximum likelihood estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a maximum likelihood estimate of the slope and the intercept. \n",
    "- Use `optimize.minimize` from `scipy` to find the maximum of the log likelihood.\n",
    "- Estimate an uncertainty by extracting the inverse of the Hessian (second derivative of the log likelihood) at the optimum. Note that the result from `optimize.minimize` will have a property `hess_inv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ba64dea91508247a4eba1ed64c00373",
     "grade": false,
     "grade_id": "cell-7af0cbb18f6b9a76",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Maximum likelihood estimate (MLE)\n",
    "#\n",
    "# The parameter vector theta = [intercept, slope]\n",
    "# Assign the two arrays with the MLE estimate and the diagonal elements of the inverse hessian\n",
    "theta_MLE = np.array([0.,0.])\n",
    "err_theta_MLE = np.array([0.,0.])\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6002de29c5842296b66c33a788e0d45c",
     "grade": true,
     "grade_id": "cell-b472548bd7e1c7c2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert not (theta_MLE==0).all(), f'The theta_MLE array should contain the MLE estimate: {theta_MLE}'\n",
    "assert not (err_theta_MLE==0).all(), 'The err_theta_MLE array should contain the MLE error estimate'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "03c8613878e119fb95404eba1da6c729",
     "grade": false,
     "grade_id": "cell-e10d1fd9547787ad",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### Step 2: Bayesian analysis\n",
    "For the Bayesian analysis we would recommend a Gaussian prior (mean=0, standard deviation=$\\sigma = 200$ km/sec) for the intercept, and a symmetric prior\n",
    "$$\n",
    "p(m|I) \\propto \\frac{1}{(1+m^2)^{3/2}}\n",
    "$$\n",
    "for the slope (the latter was used also in the fitting straight line example).\n",
    "\n",
    "- Check the trace of the MCMC sampling to make sure (visually) that you use a proper warmup period. \n",
    "- Plot the samples using the `corner` package and extract the median and 68% credibility region for the slope parameter using the marginalized distribution.\n",
    "- Use the parameter samples to create corresponding samples of our model predictions. Show the model prediction together with the data by plotting (i) the prediction using the median, point estimate for the parameters as well as (ii) a filled band corresponding to the  68% credibility region of these sampled predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b939882818ebe4429deb3fe8f145bc3",
     "grade": false,
     "grade_id": "cell-63e104271f477475",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the log prior and the log posterior\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03f05033236bfe822fdb164c560a7a06",
     "grade": false,
     "grade_id": "cell-660d28a0fb16e9d1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Perform MCMC sampling using emcee\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60dbfe547e908dd0c2fafbf8ab3869ff",
     "grade": false,
     "grade_id": "cell-c70e3a91a0d3acb9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Our choice of starting points were not optimal. It takes some time for the MCMC chains \n",
    "# to converge. We recommend to study the traces.\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "006c01f5d096a4799897735c96932958",
     "grade": false,
     "grade_id": "cell-6977cb3865029f17",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# And choose an appropriate warm-up time\n",
    "nwarmup = 0 # update this!\n",
    "\n",
    "# we'll throw-out the warmup points, collect the chains from all walkers, \n",
    "# and reshape into an array 'samples' of shape (nsamples, 2)\n",
    "samples=np.array([])\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63bac761167166954f744297948bca17",
     "grade": false,
     "grade_id": "cell-5194cb532bd5d98d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the samples using the `corner` package and extract the median and \n",
    "# 68% confidence region for the slope parameter\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e64dc95c7c8a0faf9a2b13176cbd698d",
     "grade": false,
     "grade_id": "cell-2eaf1954728a0dcb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We can use the parameter samples to create corrsponding samples of our model predictions. \n",
    "# Plot the prediction using the median, point estimate for the parameters \n",
    "# as well as a filled band corresponding to the  68% credibility region of these sampled predictions.\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "26e5312fe0521177030f1e3e1bf12ee9",
     "grade": false,
     "grade_id": "cell-379fd70f770d21ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Step 3: Bayesian Approach to Outliers (good-data/bad-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4ed2b687b8b70bb241251e8d0e65b53e",
     "grade": false,
     "grade_id": "cell-0bb194bb977f50cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "There are several Bayesian approaches to accounting for outliers. They generally involve *modifying the statistical model*. For this data, it is abundantly clear that a simple straight line is not a good fit to our data. So let's propose a more complicated model that has the flexibility to account for outliers where the experimental error bar might have been underestimated. \n",
    "\n",
    "Specifically we will assume that there is a chance that the error distribution is represented by a Lorentzian distribution rather than a Gaussian. The peaks of these two distributions are very similar in shape, but the Lorentzian has much heavier tails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "915cc6c2f0f7790948325b89c94a6153",
     "grade": false,
     "grade_id": "cell-51ed6c1361de4d8a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We will use a statistical model in which we allow each individual data point $(x_i,y_i)$ to be described by\n",
    "a mixture between a Gaussian and a Lorentzian error model: \n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "p(\\{y_i\\}~|~\\theta,\\{g_i\\},\\sigma) = & g_i \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left[\\frac{-\\left(y_M(x_i~|~\\theta) - y_i\\right)^2}{2\\sigma^2}\\right] \\\\\n",
    "&+ (1 - g_i) \\frac{1}{\\sqrt{2}\\pi \\sigma} \\frac{1}{1 + \\left[ y_M (x_i~|~\\theta) - y_i\\right]^2 / (2\\sigma^2)}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "What we've done is expanded our model with some nuisance parameters, $\\{g_i\\}$, that is a series of weights which range from 0 to 1 and encode for each point $i$ the degree to which it fits the different error models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "643175fe0d0c97851ca8920313ec2f16",
     "grade": false,
     "grade_id": "cell-a78310b301c0142b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our model is much more complicated now: it has one new parameter per data point, but these can be considered nuisance parameters that can be marginalized-out in the end.  You can use a uniform prior $U(0,1)$ for these parameters. For the theoretical model we would recommend a Gaussian prior (mean=0, standard deviation $= \\sigma = 200$ km/sec) for the intercept, and a symmetric prior\n",
    "$$\n",
    "p(m|I) \\propto \\frac{1}{(1+m^2)^{3/2}}\n",
    "$$\n",
    "for the slope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b4ebda1ef32d7ee05fcd20910b86b42",
     "grade": false,
     "grade_id": "cell-b90416dd7ee7e48f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "- Your task is to implement this likelihood and to use, e.g., the [emcee](http://dan.iel.fm/emcee/current/) MCMC package to explore the parameter space.\n",
    "- Display the joint posterior for the slope and the intercept using a corner plot (i.e. marginalize over the g-parameters). \n",
    "- Extract the median and 68% credibility region for the slope parameter using the marginalized distribution.\n",
    "- Finally, study the posterior distribution for the g-parameters. Can you conclude which data points are more likely to be outliers (i.e. underestimated tails of the error distribution)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "848be4e8a3ca310d595cd288ee63690d",
     "grade": false,
     "grade_id": "cell-3b46910823ef4bfc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Implement the log prior, likelihood, posterior\n",
    "#\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac7c5f34c0496db1dfce0ab8f1987abb",
     "grade": false,
     "grade_id": "cell-20bdcc85ad677753",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Perform the MCMC sampling\n",
    "#\n",
    "# Note that this step can be computationally costly. For the submitted version you are encouraged \n",
    "# to use less than 10,000 samples for each walker (and no more than 50 walkers)\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35fb17509d0da67e9c5ac2cee61e4fed",
     "grade": false,
     "grade_id": "cell-ca9936feec1cd0c4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Corner plot for the intercept and slope parameters\n",
    "#\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "59e0bd3d0959d9c74de7633af912fdd5",
     "grade": false,
     "grade_id": "cell-16b97e58dead861d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the posterior joint pdf for the g-parameters (using a corner plot)\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da76ed20beb88f64e5c480806e102e30",
     "grade": false,
     "grade_id": "cell-0b4a1547860a8776",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Which data points are most likely outliers?\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e70780d689f79a366aad104f7fde980e",
     "grade": false,
     "grade_id": "cell-014c22d2a511bfb7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Task (b): Error propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f852636b01bcc86e0ff17d5cfd9fd96",
     "grade": false,
     "grade_id": "cell-a0f033cdf5cbc301",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The Bayesian approach offers a straight-forward approach for dealing with (known) systematic uncertainties; namely by marginalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30462e2181bf64c49d10208b53a7168d",
     "grade": false,
     "grade_id": "cell-4b2fc9a3073b4407",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "The Hubble constant acts as a galactic ruler as it is used to measure astronomical distances according to $v = H_0 x$. An error in this ruler will therefore correspond to a systematic uncertainty in such measurements.\n",
    "\n",
    "Suppose that a particular galaxy has a measured recessional velocity $v_\\mathrm{measured} = (100 \\pm 5) \\times 10^3$ km/sec. Also assume that the Hubble constant $H_0$ is known from the analysis performed above in Step 3. Determine the posterior pdf for the distance to the galaxy assuming:\n",
    "1. A fixed value of $H_0$ corresponding to the posterior mean of the previous analysis.\n",
    "1. Using the full, sampled posterior pdf for $H_0$ from the same analysis.\n",
    "\n",
    "In this analysis we will set the intercept $m=0$ so that $v_\\mathrm{theory} = H_0 x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f61a4004c2bbb96fdac97d6bb53df902",
     "grade": false,
     "grade_id": "cell-69d116e04511ac3d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vm=100000\n",
    "sig_vm=5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff35d4af2ca596f8372f852e183dd571",
     "grade": false,
     "grade_id": "cell-32466afe3f62abc2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We assume that we can write\n",
    "$$\n",
    "v_\\mathrm{measured} = v_\\mathrm{theory} + \\delta v_\\mathrm{exp},\n",
    "$$\n",
    "where $v_\\mathrm{theory}$ is the recessional velocity according to our model, and $\\delta v_\\mathrm{exp}$ represents the noise component of the measurement. We that $\\delta v_\\mathrm{exp}$ can be described by a Gaussian pdf with mean 0 and standard deviation $\\sigma_v = 5 \\times 10^3$ km/sec. Note that we have also assumed that our model is perfect, i.e. $\\delta v_\\mathrm{theory}$ is negligible.\n",
    "\n",
    "In the following, we also assume that the error in the measurement in $v$ is uncorrelated with the uncertainty in $H_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2974dc3d790cceea71f078c6b339aed1",
     "grade": false,
     "grade_id": "cell-d8c05a2ee0b9f59a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Through application of Bayes' rule we can readily evaluate the posterior pdf $p(x|D,I)$ for the distance $x$ to the galaxy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Case 1: Fixed $H_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b75772399f9666f552c3e7cbffdb0b72",
     "grade": false,
     "grade_id": "cell-110f0e736335617e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\\begin{align}\n",
    "p(x | D,I) & \\propto p(D | x, I) p(x|I) \\\\\n",
    "& = \\frac{1}{\\sqrt{2\\pi}\\sigma_v} \\exp \\left( - \\frac{(v_\\mathrm{measured} - v_\\mathrm{theory})^2}{2\\sigma_v^2} \\right) p(x|I)\\\\\n",
    "&= \\left\\{ \\begin{array}{ll} \\frac{1}{\\sqrt{2\\pi}\\sigma_v} \\exp \\left( - \\frac{(v_\\mathrm{measured} - H_0 x)^2}{2\\sigma_v^2} \\right) & \\text{with }x \\in [x_\\mathrm{min},x_\\mathrm{max}] \\\\\n",
    "0 & \\text{otherwise},\n",
    "\\end{array} \\right.\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1b4b7dea8707ae8e420674b094874f8",
     "grade": false,
     "grade_id": "cell-2c8d4d2307ccaab1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "where $p(x|I)$ is the prior for the distance which we have assumed to be uniform, i.e. $p(x|I) \\propto 1$ in some (possibly large) region $[x_\\mathrm{min},x_\\mathrm{max}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86863627793d90af4fea423651c736b9",
     "grade": false,
     "grade_id": "cell-36ca7afdafe291ca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Find the median value for H\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b9bfb63ad91d1d51b26cb50f17ce108",
     "grade": false,
     "grade_id": "cell-08dd8ada9d3be469",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Case 2: Using the inferred pdf for $H_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "af8e9ecedcbf9250028f8d0fbfef9e2e",
     "grade": false,
     "grade_id": "cell-68267659d6b7f833",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here we use marginalization to obtain the desired posterior pdf $p(x|D,I)$ from the joint distribution of $p(x,H_0|D,I)$\n",
    "$$\n",
    "p(x|D,I) = \\int_{-\\infty}^\\infty dH_0 p(x,H_0|D,I).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "659b11236379014f5b8cb5995bf66f10",
     "grade": false,
     "grade_id": "cell-1cfd7011d138f04b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Using Bayes' rule, the product rule, and the fact that $H_0$ is independent of $x$ we find that\n",
    "$$\n",
    "p(x|D,I) \\propto p(x|I) \\int dH_0 p(H_0|I) p(D|x,H_0,I),\n",
    "$$\n",
    "which means that we have expressed the quantity that we want (the posterior for $x$) in terms of quantities that we know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dcf03adcd509ad320930da97733ec0e3",
     "grade": false,
     "grade_id": "cell-fd958a837942ce70",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The pdf $p(H_0 | I)$ is known via its $N$ samples $\\{H_{i}\\}$ generated by the MCMC sampler.\n",
    "\n",
    "This means that we can approximate \n",
    "$$\n",
    "p(x |D,I) = \\int dH_0 p(H_0|I) p(D|x,H_0,I) \\approx \\frac{1}{N} \\sum_{i=1}^N p(D | x, H_i, I)\n",
    "$$\n",
    "where we have used $p(x|I) \\propto 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca17be79077b93e57d796810347f4c4e",
     "grade": false,
     "grade_id": "cell-bed19c575616409e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Infer the distance using the two different approaches (point estimate for H_0, and using the full pdf for H_0)\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
