{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "- See deadline on the course web page\n",
    "- This problem set is solved individually. See examination rules on the course web page and the explanation of the examination procedure below.\n",
    "- The two notebooks for each problem set contain a number of basic and extra problems; you can choose which and how many to work on. The extra problems are usually more challenging.\n",
    "- Students are allowed to discuss together and help each other when solving the problems. However, every student must understand their submitted solution in the sense that they should be able to explain and discuss them with a peer or with a teacher.\n",
    "- While discussions with your peers are allowed (and even encouraged), direct plagiarism is not. Every student must reach their own understanding of submitted solutions according to the definition in the previous point.\n",
    "- The use of coding assistance from code generating artificial intelligence tools is allowed. However, every student must reach their own understanding of submitted solutions (including employed algorithms) according to the definition above.\n",
    "- Some problems include checkpoints in the form of `assert` statements. These usually check some basic functionality and you should make sure that your code passes these statements without raising an `AssertionError`. \n",
    "- Do not use other python modules than the ones included in the `environment.yml` file in the course github repo. \n",
    "\n",
    "- **Important:** The grading of problem sets requires **all** of the following actions:\n",
    "  1. Make sure to always complete **Task 0** in the header part of the notebook and that this part does not raise any `AssertionError`(s).\n",
    "  1. **Complete** the corresponding questions in Yata for every task that you have completed. This usually involves copying and pasting some code from your solution notebook and passing the code tests. You need to have a green check mark on Yata to get the corresponding points.\n",
    "  1. **Upload** your solution in the form of your edited version of this Jupyter notebook via the appropriate assignment module in Canvas (separate for basic and extra tasks). It is the code and results in your submitted notebook that is considered to be your hand-in solution.\n",
    "  1. If selected, be **available for a discussion** of your solution with one of the teachers on the Monday afternoon exercise session directly following the problem set deadline. No extra preparation is needed for these discussions apart from familiarity with your own solution. A list of randomly selected students will be published on the course web page around Monday noon. During the afternoon session that same day, students will be called in the numbered order until the end of the list (or the end of the exercise session). You must inform the responsible teacher as soon as possible following the publication of the student list if you can not be physically present at the exercise session (in which case we will have the discussion on zoom). An oral examination (on all aspects of the course) will be arranged during the exam week for students that do not show up for their discussion slot, or that fail to demonstrate familiarity with their hand-in solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "- Make sure that the **run time is smaller than a few minutes**. If needed you might have to reduce some computational tasks; e.g. by decreasing the number of grid points or sampling steps. Please ask the supervisors if you are uncertain about the run time. \n",
    "\n",
    "- Your solutions are usually expected where it says `YOUR CODE HERE` or <font color=\"red\">\"PLEASE WRITE YOUR ANSWER HERE\"</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0 \n",
    "#### (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "251105c400969a6d24e9f7ec4883888e",
     "grade": false,
     "grade_id": "cell-6f99a2583f9fb27d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "By changing the below boolean variable `student_self_assessment` to `True` you attest that:\n",
    "- All handed in solutions were produced by yourself in the sense that you understand your solutions and should be able to explain and discuss them with a peer or with a teacher.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64a7020fd0e62e5b51ef4470ae1c797f",
     "grade": false,
     "grade_id": "student-self-assessment",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "student_self_assessment = False\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d873afed15d2d3de2ef460d53fccf90f",
     "grade": true,
     "grade_id": "cell-795bedd2908899aa",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert student_self_assessment == True, 'You must assert the individual solution statements.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3 (Extra problems)\n",
    "### Learning from data [TIF285], Chalmers, Fall 2024\n",
    "\n",
    "Last revised: 30-Sep-2024 by Christian Forss√©n [christian.forssen@chalmers.se]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6874d6f0612340a685cc96f6c456b90b",
     "grade": false,
     "grade_id": "cell-bf9197ccd45cb935",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Data files are stored in\n",
    "DATA_DIR = \"DataFiles/\"\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Bayesian optimization (3 points)\n",
    "*You should have solved problem 3 to get some acquaintance with Gaussian Processes before doing this problem.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4e2b59e002bff34c8b89897fbc9d493",
     "grade": false,
     "grade_id": "cell-9cc8731d1d9f9679",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A univariate minimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to minimize the function\n",
    "$$\n",
    "f(x) = \\sin(6 x) + 0.2 x^2 - 0.7 x\n",
    "$$\n",
    "on the interval $x \\in [-5,5]$.\n",
    "\n",
    "The aim is to find the position of the minimum $x^*$ to within $\\pm 0.05$ under the constraint that we would like to make as few function evaluations as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the true function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. **Plot the true function and indicate the position of the minimum**\n",
    "Save the position of the *global* minimum in the variable `xtrue_min` (with at least two significant decimals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "389c0de5c8fabb1993fa91f7aca49e6e",
     "grade": false,
     "grade_id": "cell-128fc6a755891cc9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "xmin =  -5.\n",
    "xmax =  5.\n",
    "X_domain = np.linspace(xmin,xmax,1000)\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build your own BayesOpt algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09d3451a1e70515073b3ebf865ace1dd",
     "grade": false,
     "grade_id": "cell-a9fb5a0414774f2e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You will now implement a very different approach to minimize an objective function (this is a conventional name for the function that we are trying to minimize or maximize). The approach is known as **Bayesian optimization** and the basic idea is the following:\n",
    "* Select a few random points, evaluate the unknown function at these samples and build a **Gaussian Process regression model** for the function output in the entire input range based on this data.\n",
    "* Make a decision which point to sample next based on a so called **acquisition function** evaluated from the GP model. This decision will incorporate our current knowledge about the function including our uncertainty for its value in different regions.\n",
    "* Improve the statistical model using the new sample. Continue sampling new points according to the acquisition function.\n",
    "* If done correctly, this approach will balance **exploration** of new regions (with uncertain outputs, that might contain the minimum) and **exploitation** of the region that is currently most promising.\n",
    "* Very importantly, this method also works when you are dealing with **noisy objective functions**, i.e. when your \"measurement\" of its value at a new point in parameter space contains some random noise.\n",
    "\n",
    "Your task is to repeat the above minimization with **your own Bayesian Optimization algorithm**, that should be assembled as described below. Bayesian optimization algoritms are built into libraries such as `Scikit-optimize` and `GPyOpt`, but we will build our own simple version using functions from `numpy`, `scipy`, and `sklearn` (for building the statistical model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3407b7c7e50552a9011628c579b9d479",
     "grade": false,
     "grade_id": "cell-96c5e12768c3f0f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The pseudo-code for BayesOpt is the following (see specific hints for your implementation at the end):\n",
    "1. pick starting points $\\mathbf{x}^{(1)},\\mathbf{x}^{(2)},\\ldots \\mathbf{x}^{(k)}$, where $k \\geq 2$\n",
    "1. evaluate the objective function $f(\\mathbf{x})$ to obtain $y^{(i)}=f(\\mathbf{x}^{(i)})$ for $i=1,\\ldots,k$\n",
    "1. initialize a data vector $\\mathcal{D}_k = \\left\\{(\\mathbf{x}^{(i)},y^{(i)})\\right\\}_{i=1}^k$\n",
    "1. select a statistical model for $f(\\mathbf{x})$\n",
    "1. **For** {$n=k+1,k+2,\\ldots$}\n",
    "   1.    select $\\mathbf{x}^{(n)}$ by optimizing the acquisition function: $\\mathbf{x}^{(n)} = \\underset{\\mathbf{x}}{\\text{arg max}}\\, \\mathcal{A}(\\mathbf{x}|\\mathcal{D}_{n-1})$\n",
    "   1.    evaluate the objective function to obtain $y^{(n)}=f(\\mathbf{x}^{(n)})$\n",
    "   1.    augment the data vector $\\mathcal{D}_n = \\left\\{\\mathcal{D}_{n-1} , (\\mathbf{x}^{(n)},y^{(n)})\\right\\}$\n",
    "   1.    update the statistical model for $f(\\mathbf{x})$\n",
    "1. **end for**\n",
    "\n",
    "   Check for the minimum in the data vector that has been collected (note that it doesn't necessarily have to be the last sample).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4595c4ffcc0512968cad3f1d2232c818",
     "grade": false,
     "grade_id": "cell-8aca6d2119a76006",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Hints:**\n",
    "* You have to implement all steps in the above pseudo-code.\n",
    "* You can try with $k=2$ starting points.\n",
    "* For the statistical model you can use a GP implemented with `sklearn.gaussian_process`. Follow the examples from the lectures and the demonstration notebook.\n",
    "* Any knowledge about the objective function should be built into the covariance function. Let us assume that we don't have much information and that we use a standard RBF kernel.\n",
    "* It is recommended to constrain the RBF lengthscale so that it doesn't become unrealistic small. With `sklearn.gaussian_process`, such a constraint on the RBF kernel correlation length can be imposed using `kernels.RBF(length_scale=1.0, length_scale_bounds=(0.1, 10.))`.\n",
    "* Implement the so called Lower Confidence Bound (LCB) acquisition function for use in step 5A. Then, the acquisition function is simply: $\\mathcal{A}(\\boldsymbol{x}; | \\mathcal{D}_{n-1}) = -\\mu(\\boldsymbol{x}) + \\beta \\sigma(\\boldsymbol{x})$, where\n",
    "  * $\\mu(\\boldsymbol{x})$ is the mean of the GP model trained with the data $\\mathcal{D}_{n-1})$.\n",
    "  * $\\sigma(\\boldsymbol{x})$ is the standard deviation of the GP model trained with the data $\\mathcal{D}_{n-1})$.\n",
    "  * $\\beta$ is another hyperparameter for tuning the preference for exploring unknown regions. You can set $\\beta = 3$.\n",
    "* Remember that the GP model has to be updated (the hyperparameters re-optimized) at step 5D.\n",
    "* It is possible that you get warnings during the optimization step. In any case, you might want to set `n_restarts_optimizer`to a number larger than 0 as the optimization of GP hyperparameters might be challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "74b9b064d2216ef13a3680c8567faa26",
     "grade": false,
     "grade_id": "cell-8a3750c5a21ae7bb",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**Tasks**\n",
    "* Implement the BayesOpt minimizer\n",
    "* Assume that you are allowed a total of 100 function evaluations ($k$ of them for the starting points and $100-k$ in the loop). Are you able to find the minimum to within $\\pm 0.05$?\n",
    "* Plot the final statistical model together with the true function. Show which points that have been explored.\n",
    "* Plot also the convergence of the minimum value $\\min(y_n)$ as a function of the iteration number $n \\in \\{1, \\ldots, 100\\}$. How many iterations do you need to find the minimum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b8e04a574ad468fb5e4ee60d60b8846",
     "grade": false,
     "grade_id": "cell-51db36ede7b3bee8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the Bayesian Optimization by performing steps 1-4.\n",
    "# You might want to try with different seeds.\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00fb91a0ea73b527d9158767e57faf6d",
     "grade": false,
     "grade_id": "cell-GPplot",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can use the utility function for plotting a GP prediction with credible interval from the basic problem 3.\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e56450b2dd39447300f220cea477c68",
     "grade": false,
     "grade_id": "cell-dc6717fede403c88",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Perform the loop, i.e. repeat steps 5A-D 100-k times\n",
    "#\n",
    "# For better performance of the BayesOptimizer it is desirable to shift the grid in each iteration\n",
    "npoints=1000\n",
    "xrange=xmax-xmin\n",
    "dx=xrange/npoints\n",
    "X_domain = np.linspace(xmin+dx*np.random.uniform(),xmax,10000).reshape(-1,1)\n",
    "\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68ab0c783e2703d61efbee64bf418c92",
     "grade": false,
     "grade_id": "cell-7447939914f39046",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Print the final result. Did you find the minimum to within 0.05?\n",
    "# Print also the hyperparameters of the final statistical model\n",
    "#\n",
    "# Make three plots: \n",
    "# 1. The statistical model after the final sample.\n",
    "#    Show which points that have been sampled during the run.\n",
    "# 2. The acquisition function after the final sample.\n",
    "# 3. The minimum function output as a function of the iteration number.\n",
    "#    (i.e. what is the \"best\" output that has been found so far?)  \n",
    "#    Compare to the true minimum of f(x)\n",
    "#\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a68e4026dfb4f1c5317dfd9fedfb6c4",
     "grade": false,
     "grade_id": "cell-plotmodel",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6: A simple Bayesian binary classifier (4 points)\n",
    "*You should have solved problem 4 before doing this problem.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "899233726341683fe867bfa42b609e12",
     "grade": false,
     "grade_id": "cell-096d4fdfdae1f6b6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "#\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Additional module import statements if needed\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bayesian binary classifier that can take $(E,|m|)$ as input data and predicts a binary label (0=below $T_c$, 1=above $T_c$). \n",
    "* Use only high- and low-tempterature data for training (so that predictions for intermediate temperature data should be more difficult). Use normalized data as in Task 4.\n",
    "* The weights (and bias) of the single neuron binary classifier will be described by pdf:s that we will sample from using MCMC.\n",
    "* Use a Gaussian prior on the two weights and the bias (with ``weight decay'' $\\sigma = 1.0)$.\n",
    "* Construct the (log) likelihood as in logistic regression. \n",
    "* Use, e.g., `emcee`, for the MCMC sampling.\n",
    "* The prediction for a given input should be characterized by a pdf; i.e. the predicted probability for the state belonging to class 1 (above $T_c$) will itself be described by a pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sub-tasks**\n",
    "\n",
    "(a) Set up the training data\n",
    "\n",
    "(b) Train the Bayesian binary classifier and plot the pdf:s for the weights and bias.\n",
    "\n",
    "(c) Plot the decision boundaries for a few samples of the Bayesian binary classifier. Translate to an average decision boundary.\n",
    "\n",
    "(d) Study in particular the **prediction** of your Bayesian binary classifier for inputs $(E,|m|)$ that corresponds to:\n",
    "1. low-temperature configurations.\n",
    "1. high-temperature configurations.\n",
    "1. temperatures very close to the critical one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "433afbc2d538aa0eddded22332445a1d",
     "grade": false,
     "grade_id": "cell-ddf368b22c78b096",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "T, Es, Ms = np.loadtxt(f'{DATA_DIR}/PS3_Prob4_data.txt',unpack=True)\n",
    "\n",
    "input_data = np.column_stack((Es,Ms))\n",
    "\n",
    "Tc = 2 / np.log(1+np.sqrt(2))\n",
    "high_T = T>Tc\n",
    "\n",
    "# High-temperature = 1\n",
    "targets = high_T*np.ones_like(T,dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf81ace6679d8ab8dc5e3c6c5f67ba9f",
     "grade": false,
     "grade_id": "cell-9b1c27024fd2e825",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**Subtask (a)**: Set up the training data\n",
    "- Normalize the input data (mean=0, stddev=1)\n",
    "- Use high- (T>3.5) and low-temperature (T<1.5) data for training. \n",
    "- Plot the training data, indicate the target output 1: T>Tc with red symbols and 0: T<Tc with blue symbols.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75f51565c170ac76248c36abbbaa366d",
     "grade": false,
     "grade_id": "cell-12334284ecc2f616",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6db9603d014d01cd2e008d745e7706b1",
     "grade": false,
     "grade_id": "cell-e6723fc732d3005f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Single neuron classifier code from Problem 4 is probably needed to implement the Bayesian neuron\n",
    "\n",
    "def sigmoid(a):\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n",
    "\n",
    "def single_neuron(x, w):\n",
    "    \"\"\"\n",
    "    Single neuron prediction. \n",
    "    Column in the input x is the bias.\n",
    "    \n",
    "    Args:\n",
    "        x (array[float]): input to the neuron\n",
    "        w (array[float]):\n",
    "\n",
    "    Returns:\n",
    "        y (float): the output of the neuron\n",
    "    \"\"\"\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b01929cab66471302ffe40cff7f6ed3b",
     "grade": false,
     "grade_id": "cell-6aaa83e747ff68ea",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the log prior, likelihood, posterior\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0e258a18e4babbbf0f18ffa9a6f909f",
     "grade": false,
     "grade_id": "cell-a2016c3cbfc10935",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**Sub-task (b)**: Train the Bayesian binary classifier and plot the pdf:s for the weights and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed0b49617d78f73640305b0ac29cce91",
     "grade": false,
     "grade_id": "cell-b11765aaf06abae1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "922d7115e9f8de0afcdd02c8a9faaf7c",
     "grade": false,
     "grade_id": "cell-6c8a5990ef802901",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**Sub-task (c)**: Plot the decision boundaries for a few samples of the Bayesian binary classifier. Translate to an average decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9842b8f5aa175835be594fe02386e05",
     "grade": false,
     "grade_id": "cell-dc99504b5f4b570c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7fe271ac3a9243be3ab26292b75656e",
     "grade": false,
     "grade_id": "cell-b760d770900a2a3f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**Sub-task (d)**: Study in particular the **prediction** of your Bayesian binary classifier for inputs $(E,|m|)$ that corresponds to:\n",
    "1. low-temperature configurations.\n",
    "1. high-temperature configurations.\n",
    "1. temperatures very close to the critical one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3f1ea80a590a01503f5a37aa23ee01c",
     "grade": false,
     "grade_id": "cell-3c06bbb45fa22824",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
