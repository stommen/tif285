{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "- See deadline on the course web page\n",
    "- This problem set is solved individually. See examination rules on the course web page and the explanation of the examination procedure below.\n",
    "- The two notebooks for each problem set contain a number of basic and extra problems; you can choose which and how many to work on. The extra problems are usually more challenging.\n",
    "- Students are allowed to discuss together and help each other when solving the problems. However, every student must understand their submitted solution in the sense that they should be able to explain and discuss them with a peer or with a teacher.\n",
    "- While discussions with your peers are allowed (and even encouraged), direct plagiarism is not. Every student must reach their own understanding of submitted solutions according to the definition in the previous point.\n",
    "- The use of coding assistance from code generating artificial intelligence tools is allowed. However, every student must reach their own understanding of submitted solutions (including employed algorithms) according to the definition above.\n",
    "- Some problems include checkpoints in the form of `assert` statements. These usually check some basic functionality and you should make sure that your code passes these statements without raising an `AssertionError`. \n",
    "- Do not use other python modules than the ones included in the `environment.yml` file in the course github repo. \n",
    "\n",
    "- **Important:** The grading of problem sets requires **all** of the following actions:\n",
    "  1. Make sure to always complete **Task 0** in the header part of the notebook and that this part does not raise any `AssertionError`(s).\n",
    "  1. **Complete** the corresponding questions in Yata for every task that you have completed. This usually involves copying and pasting some code from your solution notebook and passing the code tests. You need to have a green check mark on Yata to get the corresponding points.\n",
    "  1. **Upload** your solution in the form of your edited version of this Jupyter notebook via the appropriate assignment module in Canvas (separate for basic and extra tasks). It is the code and results in your submitted notebook that is considered to be your hand-in solution.\n",
    "  1. If selected, be **available for a discussion** of your solution with one of the teachers on the Monday afternoon exercise session directly following the problem set deadline. No extra preparation is needed for these discussions apart from familiarity with your own solution. A list of randomly selected students will be published on the course web page around Monday noon. During the afternoon session that same day, students will be called in the numbered order until the end of the list (or the end of the exercise session). You must inform the responsible teacher as soon as possible following the publication of the student list if you can not be physically present at the exercise session (in which case we will have the discussion on zoom). An oral examination (on all aspects of the course) will be arranged during the exam week for students that do not show up for their discussion slot, or that fail to demonstrate familiarity with their hand-in solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "- Make sure that the **run time is smaller than a few minutes**. If needed you might have to reduce some computational tasks; e.g. by decreasing the number of grid points or sampling steps. Please ask the supervisors if you are uncertain about the run time. \n",
    "\n",
    "- Your solutions are usually expected where it says `YOUR CODE HERE` or <font color=\"red\">\"PLEASE WRITE YOUR ANSWER HERE\"</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0 \n",
    "#### (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "251105c400969a6d24e9f7ec4883888e",
     "grade": false,
     "grade_id": "cell-6f99a2583f9fb27d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "By changing the below boolean variable `student_self_assessment` to `True` you attest that:\n",
    "- All handed in solutions were produced by yourself in the sense that you understand your solutions and should be able to explain and discuss them with a peer or with a teacher.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64a7020fd0e62e5b51ef4470ae1c797f",
     "grade": false,
     "grade_id": "student-self-assessment",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "student_self_assessment = False\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d873afed15d2d3de2ef460d53fccf90f",
     "grade": true,
     "grade_id": "cell-795bedd2908899aa",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert student_self_assessment == True, 'You must assert the individual solution statements.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3 (Basic problems)\n",
    "### Learning from data [TIF285], Chalmers, Fall 2024\n",
    "\n",
    "Last revised: 30-Sep-2024 by Christian Forss√©n [christian.forssen@chalmers.se]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c532e9c42f0ac2234f7eb5f88c73672b",
     "grade": false,
     "grade_id": "cell-f3768fbcb502fd03",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Data files are stored in\n",
    "DATA_DIR = \"DataFiles/\"\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Assigning probabilities for a hundred-sided dice (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a hundred-sided dice (labeled with 1, 2, 3, ..., 100) for which you know \n",
    "\n",
    "Case 1. that the mean of a large number of rolls is $\\mu_1 = \\frac{1}{100}\\sum_{i=1}^{100} i$.\n",
    "\n",
    "Case 2. that the mean of a large number of rolls is $\\mu_2=40$ and that the standard deviation is $\\sigma_2=25$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Analytical MaxEnt derivation of the probability distributions\n",
    "**Task (see also Yata)**\n",
    "* Use the principle of maximum entropy to assign the probabilities $\\{ p_i \\}_{i=1}^{100}$ for the outcomes of a dice roll in the two different cases. Employ the method of Lagrange multipliers to derive (analytical) expressions for $p_i$.\n",
    "* Your expressions should contain the (yet undetermined) Lagrange multipliers.\n",
    "* You should use the markdown cell below to perform the analytical derivation and present the results for $p_i$ in case 1 and case 2. Start by writing down the expression for the entropy with the relevant Lagrange-multiplier terms.\n",
    "\n",
    "*Hint: There are various constraints from the known information: the normalization of the probabilities $\\mathcal{N} = \\sum_i p_i = 1$ and the mean result $\\mu=\\sum_i i p_i$. In case 2 there is also a third constraint in the variance $\\sigma^2 = \\sum_i (i-\\mu)^2 p_i$. Set the so called Lebesque measure $m_i = 1 \\; \\forall i$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3be595f8710f5cc829f8ba26e4da04dd",
     "grade": true,
     "grade_id": "cell-27f8f8a8231099be",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "* * *\n",
    "**PLEASE WRITE YOUR ANSWER HERE** \n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Determination of the Lagrange multipliers\n",
    "**Tasks (see also Yata)**\n",
    "* Determine the still unknown Lagrange multipliers; \n",
    "  - for case 1 they can be derived analytically, but it is also allowed to determine them numerically.\n",
    "  - for numerical determination you might find that it works better to use `scipy.fsolve` to find the root of $\\partial Q / \\partial p_i$ (where $Q$ is the constrained entropy) than to use `scipy.optimize` to find the maximum of $Q$.\n",
    "* Print the values of the Lagrange multipliers for each case.\n",
    "* Assign the probabilities and make a bar plot for each case. Store the probabilities in the arrays `probs_1` and `probs_2`, each of shape (100,)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a33d5c8c3512f120e3f61ae7b82b0718",
     "grade": false,
     "grade_id": "cell-3cc7232d47efdf8f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# importing modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#...\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3771843738b8aa9d29c7db6183b1f082",
     "grade": false,
     "grade_id": "cell-dd028344f374acca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We suggest to define helper functions (but you are free to use your own solution)\n",
    "#\n",
    "def pdf_1(lam, M=100, return_norm=False):\n",
    "    '''\n",
    "    Returns an array of (normalized) probabilities for a given Lagrange multiplier. Case a.\n",
    "    \n",
    "    Args:\n",
    "        lam: Lagrange multiplier (float)\n",
    "        M: number of discrete probabilities (int). Default = 100\n",
    "        return_norm: Returns the normalization factor as a second variable (boolean). Default = False\n",
    "        \n",
    "    Returns:\n",
    "        pdf: Array of shape (M,) of probabilities p_i = f_1(i,lam) / norm\n",
    "    '''\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n",
    "\n",
    "def pdf_2(lam1, lam2, mu, M=100, return_norm=False):\n",
    "    '''\n",
    "    Returns an array of (normalized) probabilities for a given Lagrange multiplier and mean. Case b.\n",
    "    \n",
    "    Args:\n",
    "        lam1: Lagrange multiplier for the mean constraint (float)\n",
    "        lam2: Lagrange multiplier for the variance constraint (float)\n",
    "        mu: mean value (float)\n",
    "        M: number of discrete probabilities (int). Default = 100\n",
    "        return_norm: Returns the normalization factor as a second variable (boolean). Default = False\n",
    "        \n",
    "    Returns:\n",
    "        pdf: Array of shape (M,) of probabilities p_i = f_2(i,lam1,lam2,mu) / norm\n",
    "    '''\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n",
    "\n",
    "def moments(pdf):\n",
    "    '''\n",
    "    Returns the first few moments of a discrete pdf.\n",
    "    \n",
    "    Args:\n",
    "        pdf: Array of shape (M,) of probabilities p_i\n",
    "        \n",
    "    Returns:\n",
    "        moments: tuple of floats with the first few moments (norm, mean, variance) \n",
    "    '''\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n",
    "    \n",
    "# and to use the principle of maximum entropy to assign the probabilities \n",
    "Msides = 100\n",
    "probs_1 = np.ones(Msides)\n",
    "probs_2 = np.ones(Msides)\n",
    "\n",
    "# Don't forget to also make a bar plot\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c6473f0438d10879dafed17370f9bfff",
     "grade": true,
     "grade_id": "cell-77f87e6d43c75b7d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "for iprobs, probs in enumerate([probs_1,probs_2]):\n",
    "    assert probs.shape == (100,), f'The array `probs` for case {iprobs} should be of shape (100,). probs.shape = {probs.shape}'\n",
    "    assert np.abs(probs.sum()-1.0)<1e-6, f'The norm of array `probs` for case {iprobs} is {probs.sum}'\n",
    "\n",
    "iside = np.arange(100)+1\n",
    "pdf_means=np.array([0.,0.])\n",
    "pdf_variances=np.array([0.,0.])\n",
    "for iprobs, probs in enumerate([probs_1,probs_2]):\n",
    "    pdf_means[iprobs] = np.sum(probs*iside)\n",
    "    pdf_variances[iprobs] = np.sum((iside-pdf_means[iprobs])**2*probs)\n",
    "assert (np.abs(pdf_means-np.array([50.50,40]))<1e-4).all(), f'The mean values are: {pdf_means}'\n",
    "assert (np.abs(pdf_variances-np.array([833.25,625.]))<1e-3).all(), f'The variances are: {pdf_variances}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Hypothesis testing using p-values (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that you are analyzing a time series of data from an experiment that is searching for an annual modulation on top of a constant background. For example, an experiment that succeeds in detecting a flux of dark matter particles reaching the earth's surface would be expected to measure a signal with an annual modulation. \n",
    "\n",
    "The time series of data consists of $N$ measurements that extends over a period of 5 years. The measurements are presented in some re-scaled, dimensionless units. From calibration measurements it is known that errors are normal distributed with a standard deviation $\\sigma = 5.0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) The chi-squared statistic and the null hypothesis\n",
    "**Tasks (see also Yata)**\n",
    "* Read the three different data sets. Each of the sets contains a constant background and might, or might not, include an annual modulation signal.\n",
    "* Compute, for each case, the chi-squared statistic: $\\chi^2 = \\sum_{i=1}^N \\frac{( y_i - \\bar{y} )^2}{\\sigma^2}$, where the average value $\\bar{y}$ is estimated from the sample $\\{ y_i \\}_{i=1}^{N}$. Implement the chi-squared statistic via the function `chi2_statistic` below.\n",
    "* Consider as null hypothesis the scenario that the signal is *constant*, albeit with an unknown amplitude. Generate a large number (M=1000) of hypothetical data sets (each with the same number of data, $N$, as the real experimental sets) from the assumption that the null hypothesis is true. The background should be set randomly for each realization and data should be generated with noise given the knowledge about statistical errors. To be specific, you can sample the background from a uniform distribution in [10,200] and the noise from a normal distribution with standard deviation $\\sigma = 5.0$..\n",
    "* Compute the chi-squared statistic for each hypothetical data set. How large fraction of the generated null-hypothesis data sets give a chi-squared statistic that is larger than the value for each of the three measured data sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3440c611c90aa540ad8e2fc9c4d07d1f",
     "grade": false,
     "grade_id": "cell-58dbfded94777e8d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# importing modules\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#...\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2afb3be1aeaf1b2ac360ad6c622e9b0",
     "grade": false,
     "grade_id": "chi2statistic",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def chi2_statistic(y,dy):\n",
    "    \"\"\"\n",
    "    Compute the chi-squared statistic for a set of data with known error and an unknown, constant background.\n",
    "\n",
    "    Since the background is unknown, the computation of the chi-squared statistic requires an estimation of the mean from the sample.\n",
    "        \n",
    "    Args:\n",
    "        y: data, array of floats \n",
    "        dy: (float) fixed error, standard deviation of a normal distribution\n",
    "        \n",
    "    Returns:\n",
    "        (float) chi-squared statistic\n",
    "    \"\"\"\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c2466195af6cd7f792f4cf7207cc923",
     "grade": false,
     "grade_id": "chi2statistic_data",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Read the data and perform the tasks listed above\n",
    "# Save the chi-squared statistic in a list `chi2_from_data`\n",
    "chi2_from_data = []\n",
    "\n",
    "for icase in range(len(amplitudes)):\n",
    "    print(f'Read data for case {icase:>2}')\n",
    "    t,y = np.loadtxt(f'{DATA_DIR}/PS3_Prob2_data{icase}.txt',unpack=True)\n",
    "    \n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be4564b7e45417b4d6822f5982e7b36c",
     "grade": false,
     "grade_id": "chi2statistic_fraction",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) p-values and null hypothesis rejection\n",
    "**Tasks (see also Yata)**\n",
    "* Make a plot of the expected $\\chi^2$ distribution for the relevant number of degrees of freedom and compare with the statistic obtained for the actual data in each case.\n",
    "* What is the *P-value* or *significance* of the data (for each case) from a frequentist point of view? Does it agree with your observation from the hypothetical null hypothesis data sets?\n",
    "* Assume that we have decided beforehand the significance level 5% for this test. Would you *reject the null hypothesis with 95% confidence* for each particular case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2a9d0135caad40d9e8f786ea7a5b828",
     "grade": false,
     "grade_id": "cell-56a4539f08238360",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Read the data and perform the tasks listed above\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Gaussian Process regression (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1afde8b7c20a9387e5c71989edc5f7a3",
     "grade": false,
     "grade_id": "cell-e7e20373ee14fef6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the following we will perform Gaussian Process (GP) regression of a simulated data set and explore\n",
    "- The prior prediction of a GP regression model with fixed hyperparameters.\n",
    "- The posterior prediction of a GP regression model with fixed hyperparameters but conditioned on training data.\n",
    "- The dependence on the choice of kernel hyperparameters for this GP prediction.\n",
    "- The optimization of kernel hyperparameters by minimization of the training data negative (log) likelihood.\n",
    "For simplicity, we will consider a problem in which both the independent data ($x$) and the targets are one-dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) The kernel and the prior GP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will use the RBF kernel with a constant signal variance ($\\sigma_f^2$) plus a diagonal, white noise term\n",
    "\n",
    "$$\n",
    "C({x}_i,{x}_j) = \\sigma_f^2 \\exp\\left(-\\frac{1}{2l^2}\n",
    "  ({x}_i - {x}_j)^2\\right) + \\sigma_\\nu^2 \\delta_{{x}_i, {x}_j}\n",
    "$$\n",
    "\n",
    "The correlation length $l$ controls the smoothness of the GP samples and $\\sigma_f$ the vertical variation. We note that this kernel is stationary and only depends on the distance $\\Delta x_{ij} \\equiv {x}_i - {x}_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kernel function gives the elements of an $N \\times N$ covariance matrix $C(\\mathbf{X}_N,\\mathbf{X}_N)$ that describes a GP for the set of positions $\\mathbf{X}_N = \\{ x_1, x_2, \\ldots, x_N \\}$. Since we will eventually use the kernel to construct different blocks of a covariance matrix, we will rather implement the computation of a general $N \\times M$ matrix $C(\\mathbf{X}_N,\\mathbf{X}'_M)$ where $\\mathbf{X}_N$ and $\\mathbf{X}'_M$ can be two different sets of positions of lengths $N$ and $M$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks (see also Yata)**\n",
    "- Complete the `RBFkernel` and `cov_RBF` functions below to include the possibility of adding a fixed white noise. Note that this term should only be added when $x_i = x_j$.\n",
    "- Complete also the `GPplot` function below to add the functionality of randomly drawing samples from the GP and plotting them together with the mean vector and credible interval (the plottings of which are already implemented).\n",
    "- Use this functionality to show the prior over functions with mean zero and a covariance matrix obtained with kernel parameters (a) $\\sigma_f^2=1.$, $l=2.$, and (b) $\\sigma_f^2=0.25$, $l=0.25$. In both cases there is a white noise $\\sigma_\\nu^2=0.01$.  Plot these prior predictions in two figure panels (one per choice of hyperparameters) for the interval $x \\in[-5,5]$. Use the newly implemented functionality of `GPplot` to show 5 samples for each prior.\n",
    "\n",
    "Useful code blocks can be found in the GP demonstration notebook in the lecture notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c21b72c6b9cb3d7f0434535bdf1dc55",
     "grade": false,
     "grade_id": "RBFkernel",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def RBFkernel(dX, alpha=np.array([1,1]), nugget=0.):\n",
    "    \"\"\"\n",
    "    The stationary RBF kernel function for a one-dimensional space. \n",
    "    \n",
    "    The distance matrix can be an arbitrarily shaped numpy array so make sure to\n",
    "    use functions like `numpy.exp` for exponentiation.\n",
    "    \n",
    "    Args:\n",
    "        dX: distance matrix. Array (N_row,N_col)\n",
    "        alpha: List-like. Hyperparameters corresponding to [RBF signal variance, RBF correlation length].\n",
    "        nugget: Fixed, diagonal white noise variance\n",
    "        \n",
    "    Returns:\n",
    "        Kernel elements. Array with same shape as dX.\n",
    "    \"\"\"\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n",
    "\n",
    "def cov_RBF(X_row, X_col, alpha=np.array([1,1]), nugget=0.):\n",
    "    \"\"\"\n",
    "    Construct the covariance matrix for input positions X_row x X_col\n",
    "        \n",
    "    Args:\n",
    "        X_row: list-like, length N_row\n",
    "        X_col: list_like, length N_col\n",
    "        alpha: hyperparameters [RBF signal variance, RBF correlation length]\n",
    "        nugget: Fixed, diagonal white noise variance\n",
    "        \n",
    "    Returns:\n",
    "        Covariance matrix. Shape (N_row, N_col)\n",
    "    \"\"\"\n",
    "    N_row = len(X_row)\n",
    "    X_row = np.array(X_row).reshape(N_row,1)\n",
    "    N_col = len(X_col)\n",
    "    X_col = np.array(X_col).reshape(N_col,1)\n",
    "\n",
    "    X_row_tile = np.tile(X_row,N_col)\n",
    "    # dX array is of shape (N_row, N_col)\n",
    "    dX = X_col.T - X_row_tile\n",
    "       \n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "327a4b73f02478de84046940882eab8d",
     "grade": false,
     "grade_id": "GPplot",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Utility function for plotting a GP prediction with credible interval\n",
    "import scipy.stats as stats\n",
    "\n",
    "def GPplot(X_new, mu_new, cov_new, ax=None, credibility=0.95, showsamples=False, numsamples=3):\n",
    "    \"\"\"\n",
    "    Plot predictions of the GP regression model.\n",
    "\n",
    "    Args:\n",
    "        X_new: New input locations. List-like of length N_new.\n",
    "        mu_new: GP regressor mean predictions. Array of shape (N_new,1).\n",
    "        cov_new: GP regressor mean predictions. Array of shape (N_new, N_new).\n",
    "        ax: Axes handle for plot. Will be created and returned if None. (default None)\n",
    "        credibility: probability mass to include in the posterior prediction (float<1, default=0.95)\n",
    "        showsamples: Include random draws from the GP in the plot. (boolean, default=False)\n",
    "        numsamples: Number of random draws from the GP shown in the plot. (int, default=3, \n",
    "            max allowed=10). Ignored if 'samples'=False\n",
    "    \n",
    "    Returns:\n",
    "        Axes handle\n",
    "    \"\"\"\n",
    "    assert credibility < 1.\n",
    "    assert credibility > 0.\n",
    "    # Determine the width of the band\n",
    "    bandwidth_factor = stats.norm.interval(credibility)[1]\n",
    "       \n",
    "    X_new = np.array(X_new).reshape(-1,1)\n",
    "        \n",
    "    credible_range_distance = bandwidth_factor * np.sqrt(np.diag(cov_new)).flatten()\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1,1)\n",
    "    ax.fill_between(X_new.flatten(), mu_new.flatten() + credible_range_distance, \\\n",
    "                    mu_new.flatten() - credible_range_distance, alpha=0.1)\n",
    "    ax.plot(X_new, mu_new, label='Mean')\n",
    "\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a09c76884ecdb05d4f6beae8ddbdbf1",
     "grade": false,
     "grade_id": "GPplotprior",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the prior over functions for hyperparameter choices (a) and (b)\n",
    "\n",
    "# Finite number of points for predictions\n",
    "Xp = np.linspace(-5, 5, 100).reshape(-1, 1)\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) The posterior and hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sklearn.gaussian_process` library is needed to solve this task. \n",
    "\n",
    "Scikit-learn provides a `GaussianProcessRegressor` for implementing [GP regression models](http://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-regression-gpr). It can be configured with [pre-defined kernels and user-defined kernels](http://scikit-learn.org/stable/modules/gaussian_process.html#gp-kernels). Kernels can be composed from several ingredient kernels. The squared exponential kernel is the `RBF` kernel in scikit-learn. The `RBF` kernel only has a `length_scale` parameter which corresponds to the $l$ parameter above. To have a $\\sigma_f$ parameter as well, we have to multiply the `RBF` kernel with a `ConstantKernel`.\n",
    "\n",
    "Tunable white noise can be included via the `WhiteKernel`. However, in these tasks we will include **fixed** noise via the `alpha` argument of the `GaussianProcessRegressor` function. This argument provides a value that is added to the diagonal of the kernel matrix during fitting. It is not, however, included in the covariance matrix when making predictions. See the remark in the tasks listed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks (see also Yata)**\n",
    "- Set up the corresponding GP regression model using `sklearn`. See the example from the GP demonstration notebook in the lecture notes. Here we will include white noise via the `alpha` argument of the `GaussianProcessRegressor` function.\n",
    "\n",
    "  **Important**: Set `optimizer=None` and `normalize_y=False` in this task.\n",
    "- Fit this GP regression model to a set of training data (to be loaded from file). Since you have set `optimizer=None`, the hyperparameters will be kept fixed when fitting the GP regression model to the data. In the final subtask, we will optimize the hyperparameters.\n",
    "- Make posterior predictions for the interval $x \\in[-5,5]$. Extract the mean vector and covariance matrix and plot these posterior predictions in two figure panels (one panel per fixed set of hyperparameters, case (a) and (b)). Use the functionality of `GPplot` to show 5 samples for each posterior. Also add the training data to the panels.\n",
    "\n",
    "  **Important** Include the argument `return_cov=True`to extract the mean vector and the covariance matrix from the `sklearn` GP regression model functionality `predict()`. Note that the white noise is not included when making predictions with `sklearn` including just an RBF kernel. Here, it has to be added manually to the returned covariance matrix by adding `nugget` to the diagonal elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "194594d3ed165070d762812345953daa",
     "grade": false,
     "grade_id": "GPRab",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e3e9aa5a3bc1df766a13e4a4b0d9006",
     "grade": false,
     "grade_id": "GPRdata",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Below is a hidden code block that is used in the solution notebook to generate and save the data. \n",
    "# \n",
    "# Please ignore the comment in this cell that says \"YOUR CODE HERE\". It gets added automatically.\n",
    "# No solution code is needed here.\n",
    "#\n",
    "filename = f'{DATA_DIR}PS3_Prob3_data.txt'\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# \n",
    "\n",
    "X_train,Y_train = np.loadtxt(filename, unpack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "46b58a076c10e684f6d8c71ea6b5c154",
     "grade": false,
     "grade_id": "GPR_plot",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Implement your own posterior prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks (see also Yata)**\n",
    "\n",
    "This subtask requires you to create your own implementation of the GP posterior prediction. You can still get points for the first two subtasks if you don't finish this one.\n",
    "- Perform a fit of hyperparmaters using functionality in `sklearn`. Again, see the example from the GP demonstration notebook in the lecture notes. You will have to set up the GP model with an optimizer. The default is `optimizer=\"fmin_l_bfgs_b\"` which should work fine. Extract and print the values of the optimized hyperparameters $\\sigma_{f,\\mathrm{opt}}^2$ and $l_\\mathrm{opt}$.\n",
    "- Plot the posterior prediction with this single, optimized set of hyperparameters for the interval $x \\in[-5,5]$. Use the functionality of `GPplot` to show 5 samples.\n",
    "\n",
    "  **Important** Include the argument `return_cov=True`to extract the mean vector and the covariance matrix from the `sklearn` GP regression model functionality `predict()`. Note that the white noise is not included in the returned covariance matrix. It has to be added manually by adding `nugget` to the diagonal elements.\n",
    "- Write your own `Predict` function (see code below). Use the functionality implemented in the first subtask.\n",
    "- Perform the fit to the training data with the optimized hyperparameters that you just determined using `sklearn`. Check that you produce the same GP posterior as with the optimized `sklearn` GP regression model. I.e., you should reproduce the same mean vector and covariance matrix for the same set of prediction positions `Xp`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd550a52e6ef57cf711694902ae74131",
     "grade": false,
     "grade_id": "GPR_fit",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1c526f857013e4fbe8e33fdf63f77bd",
     "grade": false,
     "grade_id": "GP_Predict",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def Predict(X_new, X_train, Y_train, alpha=np.array([1,1]), nugget=0.):\n",
    "    \"\"\"\n",
    "    Computes posterior distribution for a GP regression model\n",
    "    for input positions X_new.\n",
    "    \n",
    "    Args:\n",
    "        X_new: New input locations. Shape (N_new,1).\n",
    "        X_train: Training locations, shape (N_train,1).\n",
    "        Y_train: Training targets, shape (N_train,1).\n",
    "        alpha: hyperparameters [RBF signal variance, RBF correlation length]\n",
    "        nugget: Fixed, diagonal white noise variance.\n",
    "    \n",
    "    Returns:\n",
    "        Posterior mean vector (N_new, 1) and covariance matrix (N_new, N_new).\n",
    "    \"\"\"\n",
    "    X_new = np.array(X_new).reshape(-1,1)\n",
    "    X_train = np.array(X_train).reshape(-1,1)\n",
    "    Y_train = np.array(Y_train).reshape(-1,1)\n",
    "        \n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ab663dfea495670357616c7945e89f5",
     "grade": false,
     "grade_id": "GP_compare",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Neural network binary classifier (3 points)\n",
    "\n",
    "There is some physics background to this problem. However, you don't need familiarity with the Ising model to solve the data analysis problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5fa73ca1da1e358ab4be961103b55946",
     "grade": false,
     "grade_id": "cell-cbe1d36433b69afe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Background\n",
    "\n",
    "The Ising model is arguably the most famous model in condensed matter physics. It is described by the simple Hamiltonian\n",
    "\n",
    "$$\n",
    "H=‚àíJ \\sum_{\\langle i,j \\rangle} s_i s_j.\n",
    "$$\n",
    "\n",
    "Here, the $s_i=\\{‚àí1,1\\}$ are classical, binary magnetic moments (spins) sitting on a two-dimensional square lattice and the notation $\\langle i,j \\rangle$ indicates that $i,j$ are nearest neighbors and that each pair is counted only once. That is the total energy is determined by interactions between neighboring spins only. For simplicity, we will set $J=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf67220a4da7999de069dfe35644b395",
     "grade": false,
     "grade_id": "cell-e77d4ed5de44da36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Most importantly, the Ising model shows a phase transition between a paramagnetic and a ferromagnetic phase as a function of temperature. The critical temperature $T_c$ at which this change of magnetic character occurs has been calculated exactly by Lars Onsager. He found\n",
    "\n",
    "$$\n",
    "T_c = \\frac{2}{\\log \\left( 1 + \\sqrt{2} \\right) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "59d30d6730422fc95e51f7fb505e8764",
     "grade": false,
     "grade_id": "cell-95cb23f4752f4be6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Tc = 2 / np.log(1+np.sqrt(2))\n",
    "print(f\"Critical temperature: Tc = {Tc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ccf24029b1705bf15ac58f9ad507bdbb",
     "grade": false,
     "grade_id": "cell-f1f10309242ed074",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this problem we aim to reproduce this result (roughly) using a neural-network binary classifier that you implement using `tensorflow` with some ingredients that you test in python methods.\n",
    "\n",
    "The classification problem can be made more sophisticated by introducing neural networks with several layers. Results from such an analysis made it all the way into a Nature Physics publication a few years ago: [Nature Physics (2017) 13, 431‚Äì434](https://www.nature.com/articles/nphys4035)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec26e46009ac004a50c5c16527decad3",
     "grade": false,
     "grade_id": "cell-81ce45d2a47cd1a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We will start by quickly simulating the Ising model using the Monte Carlo method to obtain representative sets of spin configurations for a bunch of temperatures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba3be864cb650b8c80296591a1ebe1f7",
     "grade": false,
     "grade_id": "cell-21fdf923ad110152",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Monte Carlo simulation**\n",
    "\n",
    "The Monte Carlo method for the Ising model is very straightforward: take a random configuration of spins to begin with and propose individual spin flips until you run out of steam. To decide whether a spin should be flipped we use the Metropolis criterium\n",
    "$$\n",
    "p=\\min \\left( 1, e^{-\\beta\\Delta E} \\right)\n",
    "$$\n",
    "where $\\Delta E = E‚Ä≤‚àíE$ is the energy difference between the new (spin flipped) and the old configuration according to $H$ above and $\\beta = 1/T$ is the inverse of the temperature $T$. Since $\\Delta E$ only depends on the local environment of the spin to be flipped (nearest neighbors), we can evaluate it locally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6fadbce3015dd17d084e84907069743a",
     "grade": false,
     "grade_id": "cell-f4762b4b0ada315f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Generate spin configurations and study the phase transition**\n",
    "\n",
    "In the python file attached with this notebook we have the definition of a `Lattice` class which can be used to generate a 2D lattice for `N` spins at a temperature `T`. Here, we simply import the `Lattice` class and use the `step` method to generate a lattice after a few hundred iterations to simulate a thermalization of the lattice. \n",
    "\n",
    "At every iteration, we select $N^2$ random points to try a flip attempt. A flip attempt consists of checking the change in energy due to a flip. If it is negative or less than $e^{-E/(k_b T)}$, then perform the flip. After a few steps the lattice will thermalize.\n",
    "\n",
    "\n",
    "***To get this to work you need the `lattice.py` file, which contains the definition of `Lattice`, in the same directory as this notebook***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "079a464902e8d035b061fb51fafee5d0",
     "grade": false,
     "grade_id": "cell-166f2aa2c4c88f8c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from lattice import Lattice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c0cb3e8eb6058acc4d947059664aee4",
     "grade": false,
     "grade_id": "cell-5fb8f66b389542d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Initialize a lattice\n",
    "lat = Lattice(N=10, T=4.5)\n",
    "\n",
    "# Make 30 iterations (N**2 spin flip attempts)\n",
    "for i in range(30):\n",
    "    lat.step()\n",
    "\n",
    "print(lat.lattice) # (or even `print lat` to use the convenient repr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Illustrate some spin configurations, and plot macroscopic quantities as a function of temperature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d18b74bceeb06f5aa886e89bc408770",
     "grade": false,
     "grade_id": "cell-527860ba587f6412",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 10 x 10 lattice\n",
    "# six temperatures, 500 thermalization iterations\n",
    "# Plot the spin configurations for varying temperatures.\n",
    "# Display the magnetization too\n",
    "\n",
    "nrows, ncols = 2, 3\n",
    "fig, axs = plt.subplots(nrows, ncols)\n",
    "fig.subplots_adjust(wspace=0.6)\n",
    "\n",
    "for (ip, T) in enumerate([5.0, 4.0, 3.0, 2.3, 2.0, 1.0]):\n",
    "    lat = Lattice(N=10,T=T)\n",
    "    for k in range(500):\n",
    "        lat.step()\n",
    "\n",
    "    idx = ip // ncols, ip % ncols\n",
    "\n",
    "    axs[idx].matshow(lat.lattice,cmap=plt.cm.gray_r)\n",
    "    axs[idx].set_title(f\"T = {T:.1f}, $m$={lat.get_avg_magnetization():.1f}\")\n",
    "\n",
    "    axs[idx].get_xaxis().set_visible(False)\n",
    "    axs[idx].get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6842c83b277a1cc787c84f9e44cfb333",
     "grade": false,
     "grade_id": "cell-99ba248c6d6d446d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 10 x 10 lattice\n",
    "# 60 temperatures, 500 thermalization iterations\n",
    "\n",
    "# For a temperature range, thermalize a lattice, then\n",
    "# take a few hundred steps, recording energy and magnetization.\n",
    "# Store the means to plot next.\n",
    "# This takes about 60s with one reasonably modern computing core.\n",
    "\n",
    "# Thermalization and measurement steps\n",
    "ntherm = 500\n",
    "nmeasure = 200\n",
    "\n",
    "# points = array with (T, mean(E), abs(mean(M)), var(E))\n",
    "# with the mean and variance evaluated for a list of many temperatures\n",
    "points = []\n",
    "# Storing nmeasure / nsparse data points\n",
    "nsparse = 10\n",
    "# points_full = array with (T, E, abs(M))\n",
    "# for several different configurations per temperature\n",
    "points_full=[]\n",
    "for T in np.arange(4.0,1.0,-0.05):\n",
    "    lat = Lattice(N=10,T=T)\n",
    "    for _ in range(ntherm):\n",
    "        lat.step()\n",
    "    Es = []\n",
    "    Ms = []\n",
    "\n",
    "    for istep in range(nmeasure): \n",
    "        lat.step()\n",
    "        Es.append(lat.get_energy())\n",
    "        Ms.append(lat.get_avg_magnetization())\n",
    "        if (istep%nsparse==0):\n",
    "            points_full.append((T,Es[-1],np.abs(Ms[-1])))           \n",
    "    Es = np.array(Es)\n",
    "    Ms = np.array(Ms)\n",
    "    points.append((T,Es.mean(),np.abs(Ms.mean()),Es.var()))\n",
    "points = np.array(points)\n",
    "points_full = np.array(points_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea7b2dc399f930cff7a92d1ea78f9503",
     "grade": false,
     "grade_id": "cell-41a6828bb9991b95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the energy, magnetization, and heat capacity vs temperature\n",
    "\n",
    "fig, axs = plt.subplots(3,1,sharex=True,figsize=(8,8))\n",
    "\n",
    "axs[0].plot(points[:,0],points[:,1], label=\"E\")\n",
    "axs[0].set_ylabel(\"Energy\")\n",
    "axs[0].set_title(\"Energy vs Temperature [L = 10]\")\n",
    "\n",
    "axs[1].plot(points[:,0],points[:,2], label=\"$|m|$\")\n",
    "axs[1].set_ylabel(\"$|m|$\")\n",
    "axs[1].set_title(\"Magnetization vs Temperature [L = 10]\")\n",
    "\n",
    "# heat capacity\n",
    "# C = var(E) / ( k_B T**2)\n",
    "heat_capacity = points[:,3] / (points[:,0]**2)\n",
    "axs[2].plot(points[:,0],heat_capacity, label=\"$C$\")\n",
    "axs[2].set_xlabel(\"T\")\n",
    "axs[2].set_ylabel(\"$C$\")\n",
    "axs[2].set_title(\"Heat capacity vs Temperature [L = 10]\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.axvline(x=Tc,linestyle='-', color=\"orange\",linewidth=2.0, label=\"$T_c$ ({:.3f})\".format(Tc))\n",
    "    ax.legend(loc=\"best\", numpoints=1)\n",
    "    ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Energy vs magnetization**\n",
    "\n",
    "Blue data is low temperature ($<T_c$) and red data is high temperature ($>T_c$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a82a334461fff3dfb5a3485b415e729",
     "grade": false,
     "grade_id": "P4-data",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "high_T = points_full[:,0]>Tc\n",
    "low_T = points_full[:,0]<Tc\n",
    "\n",
    "E_M_high = points_full[high_T][:,1:]\n",
    "E_M_low = points_full[low_T][:,1:]\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.scatter(E_M_high[:,0],E_M_high[:,1],c='r',alpha=0.5,s=25)\n",
    "ax.scatter(E_M_low[:,0],E_M_low[:,1],c='b',alpha=0.5,s=25)\n",
    "ax.set_xlabel(\"$E$\")\n",
    "ax.set_ylabel(\"$|m|$\");\n",
    "\n",
    "# Below is hidden code for saving the data. No solution code is needed here\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) The sigmoid function and a single neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks (see also Yata)**\n",
    "- Complete the code below to create a sigmoid function and a single neuron prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1064cec148aed655e065fbf18c71abde",
     "grade": false,
     "grade_id": "cell-62c8fd0546101d78",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a sigmoid function and a single neuron\n",
    "#\n",
    "def sigmoid(a):\n",
    "    '''Sigmoid function with input argument a that is the activation'''\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n",
    "\n",
    "def single_neuron(x, w):\n",
    "    \"\"\"\n",
    "    Single neuron prediction, k inputs, 1 output.\n",
    "    \n",
    "    Args:\n",
    "        x (array[float]): input to the neuron. x.shape=(N,k) \n",
    "        w (array[float]): weights, w.shape=(k+1,)\n",
    "            The zero index weight is the bias term.\n",
    "\n",
    "    Returns:\n",
    "        y (float): the output of the neuron\n",
    "    \"\"\"\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4606aae6033a3a317cc92a4dd56a0f7c",
     "grade": true,
     "grade_id": "cell-4cf1dfdcd8c764f9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert sigmoid(0.0)==0.5, f'sigmoid(0.0)={sigmoid(0.0)}!=0.5'\n",
    "assert (sigmoid(np.zeros(3))==0.5*np.ones(3)).all(), f'The sigmoid function should work with arrays'\n",
    "\n",
    "x=np.array([[0,0],[1,1],[3,2]])\n",
    "w=np.array([0.,1.,-1.])\n",
    "output = single_neuron(x, w)\n",
    "assert np.abs(output[0]-0.5)<0.01, f'output[0] = {output[0]}'\n",
    "\n",
    "assert np.abs(output[1]-0.5)<0.01, f'output[1] = {output[1]}'\n",
    "assert np.abs(output[2]-0.731059)<0.01, f'output[2] = {output[2]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Create a single neuron binary classifier for critical temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1e6329535c372c67d34c53bd11d0455c",
     "grade": false,
     "grade_id": "cell-f39cfb4344d56b76",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Task (see also Yata)**\n",
    "\n",
    "- Create a binary classifier using `tensorflow.keras` that can take $(E,|m|)$ as input data and predict a binary label (0=below Tc, 1=above Tc).\n",
    "- Train this model and extract the weights (see hints).\n",
    "- You should test that your `tensorflow.keras` model gives the same predictions as your own single-neuron classifier from (a).\n",
    "- How well does it perform? Plot the decision boundary.\n",
    "\n",
    "**Hints:**\n",
    "* Normalize the input data before training / testing (mean=0, standard deviation=1).\n",
    "* Split into 70 % training data and 30% test data.\n",
    "* Build a binary classifier from a single neuron implemented using a `tensorflow.keras` model. Study the lecture notes and the Demonstration on a neural network classifier.\n",
    "* Use one input layer (no parameters) and a single neuron in the output layer with a `sigmoid` activation.\n",
    "* If you label your keras model as `model` then you might want to check with `model.summary()` that you indeed have three trainable parameters.\n",
    "* When compiling the model, use `loss='binary_crossentropy'` and the 'adam' optimizer. Training should be done with the metrics 'accuracy'.\n",
    "* You can try with different number of epochs for training, but using at least 100 is reasonable.\n",
    "* The standard practice is to monitor the loss function and/or the accuracy metric as a function of the epochs for both training and validation data.\n",
    "* The function `get_weights` can be used to extract parameters from a `tensorflow.keras` model.\n",
    "* Check visually that the final result looks reasonable by plotting the decision boundary (where Prob(T<Tc)=0.5).\n",
    "* It is instructive to re-run the model initialization and training to see how different optima are usually found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print('You have tensorflow version:', tf.__version__, '(must be at least 2.0.0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28e169ecd69962b1ae7a2f27c221ee39",
     "grade": false,
     "grade_id": "cell-f6c527ec58acb4a6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Split the data and plot\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "269b425efae606a230335f98abf7310e",
     "grade": false,
     "grade_id": "cell-0561c03180ebba8f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create the single neuron binary classifier using tensorflow.keras\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1fe9470fc65d1584bc1d6d9c1569166",
     "grade": false,
     "grade_id": "P4b-training",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Train the classifier and test the accuracy on the test set\n",
    "#\n",
    "# At the end, the optimum weights can be stored in the array 'weights_keras'\n",
    "# with element 0 the bias weight, element 1 multiplying the (scaled) energy, and 2 the (scaled) magnetization\n",
    "weights_keras = np.array([0.,0.,0.])\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a0cfc44c9133ab5e5fc652856f175d4a",
     "grade": false,
     "grade_id": "cell-3d9b3b46ff36be23",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Evaluate the final binary classifier and plot the 25%, 50%, 75% decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ed8486abff8fa961f27cbe1d423f9bb",
     "grade": false,
     "grade_id": "cell-84544bc7235b722d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the final binary classifier and plot the 25%, 50%, 75% decision boundaries\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(weights_keras)==3, \"There should be three parameters for the single neuron binary classifier\"\n",
    "assert np.all(np.abs(weights_keras)<10), \"The parameters will be of size ~1 when using normalized input data.\"\n",
    "assert weights_keras[0]>0, \"The optimum bias term will be positive (Note that input data is scaled).\"\n",
    "assert weights_keras[1]*weights_keras[2]<0, \"The two weights will be different sign after optimization (Note that input data is scaled).\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Create a neural network (single hidden layer) binary classifier for critical temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (see also Yata)**\n",
    "\n",
    "- Repeat the task from (b) to create a binary classifier that can take $(E,|m|)$ as input data and predict a binary label (0=below Tc, 1=above Tc).\n",
    "- This time use a neural network with a single hidden layer consisting of 128 nodes using the ReLU activation function.\n",
    "- Again, use one input layer (no parameters) and a single neuron in the output layer with a sigmoid activation.\n",
    "- How well does it perform? Plot the decision boundary.\n",
    "\n",
    "**Hints:**\n",
    "* Use the same data as in task (b).\n",
    "* There will be an additional layer in your `keras` model. Check the number of trainable parameters. Does it make sense?\n",
    "* You might need slightly more epochs since there are more parameters. If yuor computer manages, try with 1000 epochs.\n",
    "* Check visually the decision boundary. Does it look reasonable? What is the striking difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2042545cc4bd4a5b6a50f0ffd12d292",
     "grade": false,
     "grade_id": "P4c-model",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create the neural network binary classifier using tensorflow.keras\n",
    "# One hidden layer with 128 neurons\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f33318bb2a1c7e8bc81e3bebf49d078c",
     "grade": false,
     "grade_id": "P4c-training",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Train the classifier and test the accuracy on the test set\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe1526d703865ac5960689bbebe9a558",
     "grade": false,
     "grade_id": "P4c-decision",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the final binary classifier and plot the 25%, 50%, 75% decision boundaries\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
