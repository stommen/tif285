{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "- See deadline on the course web page\n",
    "- This problem set is solved individually. See examination rules on the course web page and the explanation of the examination procedure below.\n",
    "- The two notebooks for each problem set contain a number of basic and extra problems; you can choose which and how many to work on. The extra problems are usually more challenging.\n",
    "- Students are allowed to discuss together and help each other when solving the problems. However, every student must understand their submitted solution in the sense that they should be able to explain and discuss them with a peer or with a teacher.\n",
    "- While discussions with your peers are allowed (and even encouraged), direct plagiarism is not. Every student must reach their own understanding of submitted solutions according to the definition in the previous point.\n",
    "- The use of coding assistance from code generating artificial intelligence tools is allowed. However, every student must reach their own understanding of submitted solutions (including employed algorithms) according to the definition above.\n",
    "- Some problems include checkpoints in the form of `assert` statements. These usually check some basic functionality and you should make sure that your code passes these statements without raising an `AssertionError`. \n",
    "- Do not use other python modules than the ones included in the `environment.yml` file in the course github repo. \n",
    "\n",
    "- **Important:** The grading of problem sets requires **all** of the following actions:\n",
    "  1. Make sure to always complete **Task 0** in the header part of the notebook and that this part does not raise any `AssertionError`(s).\n",
    "  1. **Complete** the corresponding questions in Yata for every task that you have completed. This usually involves copying and pasting some code from your solution notebook and passing the code tests. You need to have a green check mark on Yata to get the corresponding points.\n",
    "  1. **Upload** your solution in the form of your edited version of this Jupyter notebook via the appropriate assignment module in Canvas (separate for basic and extra tasks). It is the code and results in your submitted notebook that is considered to be your hand-in solution.\n",
    "  1. If selected, be **available for a discussion** of your solution with one of the teachers on the Monday afternoon exercise session directly following the problem set deadline. No extra preparation is needed for these discussions apart from familiarity with your own solution. A list of randomly selected students will be published on the course web page around Monday noon. During the afternoon session that same day, students will be called in the numbered order until the end of the list (or the end of the exercise session). You must inform the responsible teacher as soon as possible following the publication of the student list if you can not be physically present at the exercise session (in which case we will have the discussion on zoom). An oral examination (on all aspects of the course) will be arranged during the exam week for students that do not show up for their discussion slot, or that fail to demonstrate familiarity with their hand-in solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "- Make sure that the **run time is smaller than a few minutes**. If needed you might have to reduce some computational tasks; e.g. by decreasing the number of grid points or sampling steps. Please ask the supervisors if you are uncertain about the run time. \n",
    "\n",
    "- Your solutions are usually expected where it says `YOUR CODE HERE` or <font color=\"red\">\"PLEASE WRITE YOUR ANSWER HERE\"</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0 \n",
    "#### (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "251105c400969a6d24e9f7ec4883888e",
     "grade": false,
     "grade_id": "cell-6f99a2583f9fb27d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "By changing the below boolean variable `student_self_assessment` to `True` you attest that:\n",
    "- All handed in solutions were produced by yourself in the sense that you understand your solutions and should be able to explain and discuss them with a peer or with a teacher.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64a7020fd0e62e5b51ef4470ae1c797f",
     "grade": false,
     "grade_id": "student-self-assessment",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "student_self_assessment = False\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d873afed15d2d3de2ef460d53fccf90f",
     "grade": true,
     "grade_id": "cell-795bedd2908899aa",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert student_self_assessment == True, 'You must assert the individual solution statements.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 1 (Extra problems)\n",
    "**Learning from data [TIF285], Chalmers, Fall 2024**\n",
    "\n",
    "Last revised: 1-Sep-2024 by Christian Forss√©n [christian.forssen@chalmers.se]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3c673ea7ef0d08851d39009c49aa547",
     "grade": false,
     "grade_id": "cell-30c16b2db381306f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Data directory\n",
    "DATA_ID = \"DataFiles/\"\n",
    "\n",
    "# import modules\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5 [extra] (3 points; manually graded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32dd0d69018dbd79b6e7c57bc7110c46",
     "grade": false,
     "grade_id": "cell-10862146b5aeebc1",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Gradient-descent variants\n",
    "*You should have solved problem 3 before doing this problem.*\n",
    "\n",
    "Write a python class for setting up and optimizing the linear-regression model used in Problem 2 and functionality for performing stochastic and mini-batch gradient descent. Start from the template provided below. You can assume that there is one independent variable (x) and one response variable (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c43adc4c37f8d06cba59da3bcfe70f4",
     "grade": false,
     "grade_id": "P5class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, xdata, ydata, polynomial_order):\n",
    "        \"\"\"\n",
    "        Setup a polynomial model.\n",
    "        \n",
    "        Args:\n",
    "            xdata: array of independent variables\n",
    "            ydata: array of response variables\n",
    "            polynomial_order: integer, degree of polynomial\n",
    "\n",
    "        Attributes:\n",
    "            xdata: data set, array of input independent variables\n",
    "            ydata: data set, array of response variables\n",
    "            num_data: integer, length of data set\n",
    "            polynomial_order: integer, degree of polynomial\n",
    "            num_parameters: total number of model parameters\n",
    "            data_design_matrix: array of shape (len(xdata), num_parameters)\n",
    "                design_matrix corresponding to data set\n",
    "            theta: current values of model parameters, array of length `num_parameters`\n",
    "            \n",
    "        \"\"\"\n",
    "        self.xdata = np.array(xdata).reshape(-1, 1)\n",
    "        self.ydata = np.array(ydata).reshape(-1, 1)\n",
    "        assert self.xdata.shape[0]==self.ydata.shape[0]\n",
    "        self.num_data = self.ydata.shape[0]\n",
    "        self.polynomial_order = polynomial_order\n",
    "        self.num_parameters = polynomial_order + 1\n",
    "        assert self.num_parameters <= self.num_data\n",
    "        self.data_design_matrix = self.create_polynomial_design_matrix(self.xdata)\n",
    "        self.theta = np.random.uniform(size=self.num_parameters)\n",
    "\n",
    "    def create_polynomial_design_matrix(self, xdata):\n",
    "        \"\"\"\n",
    "        Create a design matrix for a polynomial model, and return it.\n",
    "        \n",
    "        Args:\n",
    "            xdata: array of independent variables\n",
    "\n",
    "        Returns:\n",
    "            design_matrix: design_matrix, array of shape (len(xdata), num_parameters)\n",
    "        \"\"\"\n",
    "        # \n",
    "        # YOUR CODE HERE\n",
    "        # \n",
    "\n",
    "    def predict(self, xpred):\n",
    "        \"\"\"\n",
    "        Perform a prediction, y_pred = X_pred \\theta.\n",
    "\n",
    "        Args:\n",
    "            xpred: array of independent variables\n",
    "\n",
    "        Returns:\n",
    "            ypred: array of predicted response variables\n",
    "        \"\"\"\n",
    "        # \n",
    "        # YOUR CODE HERE\n",
    "        # \n",
    "\n",
    "    def solve_normal_equation(self):\n",
    "        \"\"\"\n",
    "        Solves the normal equation and updates the parameters theta.\n",
    "\n",
    "        Attributes:\n",
    "            theta_fit: Optimized parameters from solving the normal equation (array)\n",
    "        \"\"\"\n",
    "        # \n",
    "        # YOUR CODE HERE\n",
    "        # \n",
    "\n",
    "    def gradient(self, xdata, ydata, theta):\n",
    "        \"\"\"\n",
    "        Setup the cost function with the input data and computes its gradient with regards to the model parameters at the specific point.\n",
    "\n",
    "        Args:\n",
    "            xdata: array of independent variables that defines the cost function\n",
    "            ydata: array of response variables that defines the cost function\n",
    "            theta: current values of model parameters\n",
    "\n",
    "        Returns:\n",
    "            grad_theta: array, gradient vector\n",
    "        \"\"\"\n",
    "        # \n",
    "        # YOUR CODE HERE\n",
    "        # \n",
    "\n",
    "    def bgd_optimize(self, theta_start, n_epochs=1000, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Find optimized parameters using (batch) gradient descent.\n",
    "\n",
    "        Uses all data in `xdata`, `ydata` attributes and starts from the position theta_start.\n",
    "    \n",
    "        Args:\n",
    "            theta_start: starting guess for model parameters\n",
    "            n_epochs: Number of epochs (default 1000) (integer)\n",
    "            learning_rate: learning rate (default 0.1) (float)\n",
    "                  \n",
    "        Attributes:\n",
    "            theta_bgd: Optimized parameters from batch gradient descent (array)\n",
    "            theta_bgd_history: array of shape (n_epochs+1, self.num_parameters)\n",
    "                History of parameter updates. Includes the start vector.\n",
    "        \"\"\"\n",
    "        # \n",
    "        # YOUR CODE HERE\n",
    "        # \n",
    "\n",
    "    def sgd_optimize(self, theta_start, n_epochs=1000, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Find optimized parameters using stochastic gradient descent.\n",
    "\n",
    "        Uses all data in `xdata`, `ydata` attributes and starts from the position theta_start.\n",
    "    \n",
    "        Args:\n",
    "            theta_start: starting guess for model parameters\n",
    "            n_epochs: Number of epochs (default 1000) (integer)\n",
    "            learning_rate: learning rate (default 0.1) (float)\n",
    "                  \n",
    "        Attributes:\n",
    "            theta_sgd: Optimized parameters from stochastic gradient descent (array)\n",
    "            theta_sgd_history: array of shape (n_epochs+1, self.num_parameters)\n",
    "                History of parameter updates. Includes the start vector.\n",
    "        \"\"\"\n",
    "        # \n",
    "        # YOUR CODE HERE\n",
    "        # \n",
    "    \n",
    "    def mbgd_optimize(self, theta_start, n_epochs=1000, learning_rate=0.01, batch_size=0.2):\n",
    "        \"\"\"\n",
    "        Find optimized parameters using stochastic gradient descent.\n",
    "\n",
    "        Uses all data in `xdata`, `ydata` attributes and starts from the position theta_start.\n",
    "    \n",
    "        Args:\n",
    "            theta_start: starting guess for model parameters\n",
    "            n_epochs: Number of epochs (default 1000) (integer)\n",
    "            learning_rate: learning rate (default 0.1) (float)\n",
    "            batch_size: maximum fraction of data in each mini batch (default 0.2) (float < 1.0)\n",
    "                  \n",
    "        Attributes:\n",
    "            theta_mbgd: Optimized parameters from mini-batch gradient descent (array)\n",
    "            theta_mbgd_history: array of shape (n_epochs+1, self.num_parameters)\n",
    "                History of parameter updates. Includes the start vector.\n",
    "        \"\"\"\n",
    "        # \n",
    "        # YOUR CODE HERE\n",
    "        # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Batch and stochastic gradient descent\n",
    "\n",
    "Implement both batch, stochastic, and mini-batch gradient descent and use these methods to find the best fit parameters of a quadratic model. Make sure that you also save the convergence path, i.e., how the parameters change as a function of iteration number. \n",
    "\n",
    "Concerning batch gradient descent you can re-use the methods that you implemented in Problem 2, with relevant modifications to incorporate them in the `LinearRegression` class. Note the use of `n_epochs` as an argument to make results comparable between different algorithms.\n",
    "\n",
    "* You might want to tune the learning hyperparameter as it can be smaller when you do many parameter updates for each epoch.\n",
    "* Do 1000 epochs (each epoch corresponding to using all instances of data once).\n",
    "* Use a mini-batch size of 20% for MBGD.\n",
    "* Start from `[0., 0., 0.]`.\n",
    "* Compare with the solution from Problem 2.\n",
    "* hint: the `permutation` method from `numpy.random` might be useful. The recommended procedure for random number sampling is to create a `Generator` instance with `numpy.random.default_rng` and call the relevant methods as attributes to this instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aea29ca2195397e523e8c3fc3450f223",
     "grade": false,
     "grade_id": "P5bestfits",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the data from Problem 2, initialize the quadratic model, and find the best fit parameters using\n",
    "# Normal equation, BGD, SGD, MBGD\n",
    "# Compare the final parameter values\n",
    "\n",
    "filename = f'{DATA_ID}PS1_Prob3_data.txt'\n",
    "data = np.loadtxt(filename)\n",
    "xdata = data[:,0]\n",
    "ydata = data[:,1]\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Convergence\n",
    "Compare the convergence pattern for the BGD, SGD and MBGD, i.e. plot the path towards the optimal set of parameters as a function of the epochs. \n",
    "- You can start from $\\theta=(0,0,0)$ with all three GD algorithms.\n",
    "- Note that the path is in 3D (since there are three parameters in our quadratic model). You should plot the three different 2D projections of the path (you might want to select the plot range such that the difference convergence behaviors can be seen).\n",
    "- Indicate also the true optimum (as found by solving the normal equation).\n",
    "\n",
    "For full points you should also comment on your results. \n",
    "- What are the main features of the different algorithms? \n",
    "- Did you use different learning rates for the different algorithms (if so, why?). \n",
    "\n",
    "Write your commentary in a markdown cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51121a93790541e5e3d90fd46e7c6721",
     "grade": false,
     "grade_id": "P5convergence",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6 [extra] (3 points; manually graded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "Repeat Problem 5 but implement also the Adam optimizer (see the lecture notes) into the `LinearRegression` class.\n",
    "\n",
    "For full points you should also comment on your results. \n",
    "- What behaviour do you observe for the convergence of the Adam optimizer? \n",
    "- What behaviour do you observe for the learning rate of the Adam optimizer?\n",
    "\n",
    "Write your commentary in a markdown cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f98b423be322743ea426a77f2c29f30e",
     "grade": false,
     "grade_id": "P6class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, xdata, ydata, polynomial_order):\n",
    "        \"\"\"\n",
    "        Setup a polynomial model.\n",
    "        \n",
    "        Args:\n",
    "            xdata: array of independent variables\n",
    "            ydata: array of response variables\n",
    "            polynomial_order: integer, degree of polynomial\n",
    "\n",
    "        Attributes:\n",
    "            xdata: data set, array of input independent variables\n",
    "            ydata: data set, array of response variables\n",
    "            num_data: integer, length of data set\n",
    "            polynomial_order: integer, degree of polynomial\n",
    "            num_parameters: total number of model parameters\n",
    "            data_design_matrix: array of shape (len(xdata), num_parameters)\n",
    "                design_matrix corresponding to data set\n",
    "            theta: current values of model parameters, array of length `num_parameters`\n",
    "            \n",
    "        \"\"\"\n",
    "        self.xdata = np.array(xdata).reshape(-1, 1)\n",
    "        self.ydata = np.array(ydata).reshape(-1, 1)\n",
    "        assert self.xdata.shape[0]==self.ydata.shape[0]\n",
    "        self.num_data = self.ydata.shape[0]\n",
    "        self.polynomial_order = polynomial_order\n",
    "        self.num_parameters = polynomial_order + 1\n",
    "        assert self.num_parameters <= self.num_data\n",
    "        self.data_design_matrix = self.create_polynomial_design_matrix(self.xdata)\n",
    "        self.theta = np.random.uniform(size=self.num_parameters)\n",
    "\n",
    "    def create_polynomial_design_matrix(self, xdata):\n",
    "        \"\"\"\n",
    "        Create a design matrix for a polynomial model, and return it.\n",
    "        \n",
    "        Args:\n",
    "            xdata: array of independent variables\n",
    "\n",
    "        Returns:\n",
    "            design_matrix: design_matrix, array of shape (len(xdata), num_parameters)\n",
    "        \"\"\"\n",
    "        # \n",
    "        # YOUR CODE HERE\n",
    "        # \n",
    "\n",
    "    def predict(self, xpred):\n",
    "        \"\"\"\n",
    "        Perform a prediction, y_pred = X_pred \\theta.\n",
    "\n",
    "        Args:\n",
    "            xpred: array of independent variables\n",
    "\n",
    "        Returns:\n",
    "            ypred: array of predicted response variables\n",
    "        \"\"\"\n",
    "        # \n",
    "        # YOUR CODE HERE\n",
    "        # \n",
    "\n",
    "    def solve_normal_equation(self):\n",
    "        \"\"\"\n",
    "        Solves the normal equation and updates the parameters theta.\n",
    "\n",
    "        Attributes:\n",
    "            theta_fit: Optimized parameters from solving the normal equation (array)\n",
    "        \"\"\"\n",
    "        # \n",
    "        # YOUR CODE HERE\n",
    "        # \n",
    "\n",
    "    def gradient(self, xdata, ydata, theta):\n",
    "        \"\"\"\n",
    "        Setup the cost function with the input data and computes its gradient with regards to the model parameters at the specific point.\n",
    "\n",
    "        Args:\n",
    "            xdata: array of independent variables that defines the cost function\n",
    "            ydata: array of response variables that defines the cost function\n",
    "            theta: current values of model parameters\n",
    "\n",
    "        Returns:\n",
    "            grad_theta: array, gradient vector\n",
    "        \"\"\"\n",
    "        # \n",
    "        # YOUR CODE HERE\n",
    "        # \n",
    "\n",
    "    def bgd_optimize(self, theta_start, n_epochs=1000, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Find optimized parameters using (batch) gradient descent.\n",
    "\n",
    "        Uses all data in `xdata`, `ydata` attributes and starts from the position theta_start.\n",
    "    \n",
    "        Args:\n",
    "            theta_start: starting guess for model parameters\n",
    "            n_epochs: Number of epochs (default 1000) (integer)\n",
    "            learning_rate: learning rate (default 0.1) (float)\n",
    "                  \n",
    "        Attributes:\n",
    "            theta_bgd: Optimized parameters from batch gradient descent (array)\n",
    "            theta_bgd_history: array of shape (n_epochs+1, self.num_parameters)\n",
    "                History of parameter updates. Includes the start vector.\n",
    "        \"\"\"\n",
    "        # \n",
    "        # YOUR CODE HERE\n",
    "        # \n",
    "\n",
    "    def sgd_optimize(self, theta_start, n_epochs=1000, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Find optimized parameters using stochastic gradient descent.\n",
    "\n",
    "        Uses all data in `xdata`, `ydata` attributes and starts from the position theta_start.\n",
    "    \n",
    "        Args:\n",
    "            theta_start: starting guess for model parameters\n",
    "            n_epochs: Number of epochs (default 1000) (integer)\n",
    "            learning_rate: learning rate (default 0.1) (float)\n",
    "                  \n",
    "        Attributes:\n",
    "            theta_sgd: Optimized parameters from stochastic gradient descent (array)\n",
    "            theta_sgd_history: array of shape (n_epochs+1, self.num_parameters)\n",
    "                History of parameter updates. Includes the start vector.\n",
    "        \"\"\"\n",
    "        # \n",
    "        # YOUR CODE HERE\n",
    "        # \n",
    "    \n",
    "    def mbgd_optimize(self, theta_start, n_epochs=1000, learning_rate=0.01, batch_size=0.2):\n",
    "        \"\"\"\n",
    "        Find optimized parameters using stochastic gradient descent.\n",
    "\n",
    "        Uses all data in `xdata`, `ydata` attributes and starts from the position theta_start.\n",
    "    \n",
    "        Args:\n",
    "            theta_start: starting guess for model parameters\n",
    "            n_epochs: Number of epochs (default 1000) (integer)\n",
    "            learning_rate: learning rate (default 0.1) (float)\n",
    "            batch_size: maximum fraction of data in each mini batch (default 0.2) (float < 1.0)\n",
    "                  \n",
    "        Attributes:\n",
    "            theta_mbgd: Optimized parameters from mini-batch gradient descent (array)\n",
    "            theta_mbgd_history: array of shape (n_epochs+1, self.num_parameters)\n",
    "                History of parameter updates. Includes the start vector.\n",
    "        \"\"\"\n",
    "        # \n",
    "        # YOUR CODE HERE\n",
    "        # \n",
    "\n",
    "    def adam_optimize(self, theta_start, n_epochs=1000, learning_rate=0.01, gamma1=0.9, gamma2=0.999, eps=10e-8):\n",
    "        # \n",
    "        # YOUR CODE HERE\n",
    "        # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ed3c0f0fcd6df4999ac1bcb5333cc4b",
     "grade": false,
     "grade_id": "P6results",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "filename = f'{DATA_ID}PS1_Prob3_data.txt'\n",
    "data = np.loadtxt(filename)\n",
    "xdata = data[:,0]\n",
    "ydata = data[:,1]\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f03c835ad8ab58d9537c63888bd9a02",
     "grade": false,
     "grade_id": "P6plots",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
